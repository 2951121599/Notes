{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回归分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.0 目录\n",
    "1.回归模型\n",
    "\n",
    "2.普通最小二乘法\n",
    "\n",
    "3.推断和假设检验\n",
    "\n",
    "4.多变量的多种形式\n",
    "\n",
    "5.假设误差分析\n",
    "\n",
    "6.异方差下的回归分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 从建模的目的看回归\n",
    "\n",
    "回归分析与分类分析都是一种基于统计模型的统计分析方法。它们都研究因变量（被解释变量）与自变量（解释变量）之间存在的潜在关系，并通过统计模型的形式将这些潜在关系进行显式的表达。不同的是，回归分析中因变量是连续变量，如工资、销售额；而分类分析中因变量是属性变量，如判断邮件“是or否”为垃圾邮件。\n",
    "\n",
    "回归分析的过程本质上一种建模过程。统计建模的主要任务有二：预测与推断。\n",
    "\n",
    "所谓预测，就是利用一个训练完毕的模型$\\hat{f}$，根据输入的自变量$X$获得对应的输出$Y$。在预测任务中，如果模型$\\hat{f}$可以准确地提供预测，那么$\\hat{f}$是什么形式并不重要，而如果$\\hat{f}$的形式非常复杂且难以解释，我们可以将之称为黑盒模型(Black Box)。事实上，当前具有强大预测性能的模型大多都是黑盒模型，如强大的Xgboost机器学习算法以及各种深度学习算法，它们的模型可解释性差，我们难以解释其中一些参数的含义与统计性质。\n",
    "\n",
    "与预测相对应的另一任务便是推断。在很多情况下，我们对当$X_1,X_2,\\cdots ,X_p$变化时**如何影响**$Y$更感兴趣，此时，我们估计模型$\\hat{f}$的目的不是为了预测$Y$，而是想明白两者之间的关系，更深层次地讲，我们想要知道模型内各种参数的数值与统计推断性质等等。在这种情况下，模型的可解释性就非常重要了，而通常我们在推断任务中最常使用的模型正是线性回归模型。举一个例子，在研究各因素对商品销售量的场景中，我们会更关注以下问题：哪类媒体对销量有直接的贡献？增加电视广告费用能对销售量带来多少程度的增加？等等，这就是典型的推断问题。\n",
    "\n",
    "弄清楚了预测与推断的区别，我们重新审视一下回归分析：回归分析更加注重对因变量与自变量之间潜在关系的推断，所使用的统计模型也相对简单（一般为线性模型），如果你在比赛中需要分析各变量间的潜在相关关系，便可以考虑使用回归分析。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 回归思想与一般回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 横截面数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "横截面数据是回归分析最主要的分析数据类型，它可以视为在**同一时间点（或抽样时间差异可以被忽略）**上对**多个抽样个体**的观测数据。通常，我们记第$i$个个体的观测数据为$(x_i,y_i)$。如果以抽样时间点与抽样个体数目为维度划分数据类型，除了横截面数据外，还有时间序列数据以及面板数据。时间序列数据为单个个体在不同时间点上的观测数据，而面板数据则是多个个体在不同时间点上的观测数据。对时间序列数据的分析需要用到时间序列分析的知识，对面板数据的分析则是高级计量经济学的内容。三者的区别如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<img src=\"./images/横截面数据.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 回归思想——条件均值建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "横截面数据最重要的一个特征，就是我们可以将采集的数据$(x_1,y_1),(x_2,y_2),…,(x_n,y_n)$近似视为来自一个潜在总体的随机样本，即假设\n",
    "\n",
    "$$\n",
    "E(y \\mid x), \\operatorname{Var}(y \\mid x)\n",
    "$$\n",
    "而回归正是利用条件均值$E(y \\mid x)$来刻画$x$与$y$的关系，回归建模的本质也正是“条件均值的建模”。那么，怎么理解条件均值建模呢？我们举一个不典型的例子帮助大家理解。\n",
    "\n",
    "假设某个样本量为100的数据集中，自变量$x$有1,2,3,4,5五个值，样本的因变量$y$都来自以其自变量为均值，方差为1的正态分布。我们想要刻画因变量与自变量之间的变化关系，就要找出可以代表各种类样本内（在此例中以自变量为划分依据）共性的特征，用这些特征来描绘变化关系。最直观也是最简单的特征就是条件均值，即给定$x$的条件下样本的均值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'E(Y|X)')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=list()\n",
    "y=list()\n",
    "for i in [1,2,3,4,5]:\n",
    "    y_norm=stats.norm.rvs(i,1,20,random_state=i).tolist()\n",
    "    y.extend(y_norm)\n",
    "    x1=np.ones(20)*i\n",
    "    x1=x1.tolist()\n",
    "    x.extend(x1)\n",
    "\n",
    "data={'x':x,'y':y}\n",
    "df=pd.DataFrame(data)\n",
    "\n",
    "sns.regplot(x='x',y='y',data=df)\n",
    "plt.title('E(Y|X)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据上图我们可以看出，$E(y|x=1)=1$,$E(y|x=2)=2$,…,$E(y|x=x_0)=x_0$。通过条件均值，我们可以推断出$x$与$y$的关系可以用模型$y=x+u$来刻画，其中，$u$被称为随机误差，可理解为：除$x$外，其他影响$y$取值的因素。\n",
    "\n",
    "使用模型$y=x+u$刻画了$x$与$y$的关系，这说明了在这个数据集中我们将模型设定为了\n",
    "$$\n",
    "y=x+u\n",
    "$$\n",
    "事实上，如果我们将上述公式中的$x$泛化成条件均值$E(y|x)$，那么我们就能得到最一般的回归模型\n",
    "$$\n",
    "y=E(y|x)+u\n",
    "$$\n",
    "这也就意味着，所谓回归模型的建模，**本质上就是条件均值建模**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 线性回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 线性模型形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的学习中我们介绍了回归模型的一般形式。在实际建模中，为了有效的估计，我们必须对模型中$m(x)$的形式进行具体的假定。在所有模型假定形式中，线性回归模型是最常用假定形式，也是回归分析中最重要的模型，是本次课程重点讲解的内容。\n",
    "\n",
    "线性模型假设有：\n",
    "$$\n",
    "m(x)=\\beta_{0}+\\beta_{1} x_{1}+\\cdots+\\beta_{p} x_{p}\n",
    "$$\n",
    "于是，线性回归模型可表示为：\n",
    "$$\n",
    "y=\\beta_{0}+\\beta_{1} x_{1}+\\cdots+\\beta_{p} x_{p}+u, \\quad E\\left(u \\mid x_{1}, \\cdots, x_{p}\\right)=0\n",
    "$$\n",
    "回归分析主要研究如何有效地估计模型中的参数$\\hat{\\beta}_i$，并利用模型进行推断与预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 从简单线性回归到多元线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 用简单线性回归理解对模型的解释**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为大家快速理解线性回归模型，我们先假设$x$是一维的，即只考虑一个因素对$y$的影响，此时亦称模型为简单线性回归，形式为\n",
    "$$\n",
    "y=\\beta_{0}+\\beta_{1} x+u, \\quad E(u \\mid x)=0\n",
    "$$\n",
    "$\\beta_{0}$是截距项，可以理解为$x=0$时$y$的期望值，一般情况下，如果我们回归的任务是推断，则截距通常不重要；\n",
    "<br>\n",
    "$\\beta_{1}=\\frac{\\Delta m(x)}{\\Delta x}$，可理解为$x$每增加一个单位，$y$**平均**增加$\\beta_1$个单位。\n",
    "\n",
    "此后，我们将默认模型含有$E(u|x)=0$的设定（因为只有这样模型才代表回归模型），该条件不再以书面形式写出。\n",
    "\n",
    "我们举一个例子帮助大家理解：\n",
    "\n",
    "**Example1.** 假设大学成绩colGPA与大学测验水平ACT间关系为\n",
    "$$\n",
    "\\text { colGPA }=\\beta_{0}+\\beta_{1} \\text { hsGPA }+u\n",
    "$$\n",
    "$\\beta_1$系数的解释为：每增加1单位大学测验水平，大学成绩会增加$\\beta_1$个单位；由于该模型中自变量只有高中成绩，而大学成绩水平肯定还受其他因素影响，因此该模型中的随机误差包含了如高中成绩、自主学习能力等因素。\n",
    "\n",
    "注意：设定$E(u|x)=0$的存在暗含了**在该模型中**高中测验成绩、自主学习能力等因素与自变量大学测验水平无关，但这在**实际问题中**未必成立。而一旦它们存在相关性，就意味着模型假设不符合实际情况，模型估计的有效性与准确性也将受到影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 进行全面的回归建模——多元线性回归**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单纯的简单线性模型有很大的局限性，原因有二：\n",
    "\n",
    "1、在实际问题中，因变量$y$通常受多个因素影响，这些因素之间可能彼此之间存在线性相关性（后续的学习中我们将这种现象称为多重共线性），而默认假设$E(u|x)=0$的直接推论(推论2)就是其他影响因素与$x$线性无关，显然不一定符合实际情况。\n",
    "\n",
    "2、如果我们想推断一个变量对另一个变量的因果关系，就要保持尽可能多的其他因素的不变，因此需要尽量把关键因素纳入到回归模型当中，这样便可以控制多个变量，查看某个特定变量变化对自变量的影响。\n",
    "\n",
    "因此在实际问题中，我们更多地使用多元线性回归。一般的多元线性回归模型可写成：\n",
    "$$\n",
    "y=\\beta_{0}+\\beta_{1} x_{1}+\\cdots+\\beta_{k} x_{k}+u\n",
    "$$\n",
    "$u$依旧为随机误差项，它表示除$x_1$,…,$x_k$以外的其他因素对因变量$y$的影响，且同样满足假设\n",
    "$$\n",
    "E\\left(u \\mid x_{1}, \\cdots, x_{k}\\right)=0\n",
    "$$\n",
    "$\\beta_i=\\frac{\\partial m\\left( x \\right)}{\\partial x_i}$是回归函数对变量$x_i$的偏导数，它被解释为**在保持其他自变量不变的情况下，$x_i$每增加一单位，$y$平均增加$\\beta_i$个单位**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· Example2.** 依旧以上面的大学成绩例子为例，这一次我们增加一个高中成绩hsGPA变量，此时模型变为\n",
    "$$\n",
    "\\mathrm{colGPA}=\\beta _0+\\beta _1\\mathrm{hsGPA}+\\beta _2\\mathrm{ACT}+u\n",
    "$$\n",
    "在模型增加了一个我们认为非常重要的变量后，模型的估计会产生怎样的变化呢？我们使用python对该例的数据集进行回归分析，比较两种模型的区别。具体的python实现过程我们将稍后介绍，大家只需要关注这里的结果即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>ACT</th>\n",
       "      <th>hsGPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     const   ACT  hsGPA\n",
       "0      1.0  21.0    3.0\n",
       "1      1.0  24.0    3.2\n",
       "2      1.0  26.0    3.6\n",
       "3      1.0  27.0    3.5\n",
       "4      1.0  28.0    3.9\n",
       "..     ...   ...    ...\n",
       "136    1.0  23.0    3.3\n",
       "137    1.0  25.0    3.6\n",
       "138    1.0  21.0    3.4\n",
       "139    1.0  26.0    3.7\n",
       "140    1.0  28.0    3.3\n",
       "\n",
       "[141 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 加载数据\n",
    "gpa1=pd.read_stata('./data/gpa1.dta')\n",
    "\n",
    "# 在数据集中提取自变量\n",
    "X1=gpa1.ACT\n",
    "X2=gpa1[['ACT','hsGPA']]\n",
    "# 提取因变量\n",
    "y=gpa1.colGPA\n",
    "\n",
    "# 为自变量增添截距项\n",
    "X1=sm.add_constant(X1)\n",
    "X2=sm.add_constant(X2)\n",
    "display(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>2.402979</td>\n",
       "      <td>8.798591e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACT</th>\n",
       "      <td>0.027064</td>\n",
       "      <td>1.389927e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         params        pvalue\n",
       "const  2.402979  8.798591e-16\n",
       "ACT    0.027064  1.389927e-02"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>1.286328</td>\n",
       "      <td>0.000238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACT</th>\n",
       "      <td>0.009426</td>\n",
       "      <td>0.383297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hsGPA</th>\n",
       "      <td>0.453456</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         params    pvalue\n",
       "const  1.286328  0.000238\n",
       "ACT    0.009426  0.383297\n",
       "hsGPA  0.453456  0.000005"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 拟合两个模型\n",
    "gpa_lm1=sm.OLS(y,X1).fit()\n",
    "gpa_lm2=sm.OLS(y,X2).fit()\n",
    "\n",
    "# 输出两个模型的系数与对应p值\n",
    "p1=pd.DataFrame(gpa_lm1.pvalues,columns=['pvalue'])\n",
    "c1=pd.DataFrame(gpa_lm1.params,columns=['params'])\n",
    "p2=pd.DataFrame(gpa_lm2.pvalues,columns=['pvalue'])\n",
    "c2=pd.DataFrame(gpa_lm2.params,columns=['params'])\n",
    "display(c1.join(p1,how='right'))\n",
    "display(c2.join(p2,how='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们发现，变量ACT在两个回归模型中的系数并不一致，且其在单独回归时变量显著，但增添了变量hsGPA后变得不显著。这说明多个变量共同回归绝不等同于多个变量各自进行单变量回归，且在后面的课程中我们会知道将多个重要变量都纳入回归模型的重要性。总之，大家在此只需要知道：**多元线性回归非常重要，后续的学习也将围绕多元线性回归展开！**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.普通最小二乘法\n",
    "\n",
    "线性回归模型里的参数是使用什么方法计算出来的呢？线性回归中最常用、最经典的系数估计方法——普通最小二乘估计法(Ordinary Least Squares, OLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 OLS估计的思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用一个关于储蓄与收入间关系的例子解释ols估计的思想。将储蓄savings视作因变量$y$，将收入income视作自变量$x$，由于只有一个自变量，因此可用简单线性回归模型假设两者关系为$y=\\beta_{0}+\\beta_{1} x+u$，即一条带有趋势与截距的直线。那么，这条直线应该“长成”怎样才算是一条“好的直线”呢？直观上看，最佳的拟合直线应该尽可能的贴合样本点，如下图所示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='./images/ols.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直线尽可能贴合样本点，意味着在所有直线当中，我们要选出一条离所有样本点距离的总和最小的直线。那么，这个距离该如何衡量？我们将模型回归参数分别记为$\\hat{\\beta}_{0}$，$\\hat{\\beta}_{1}$，并定义$\\hat{y}_{i}=\\hat{\\beta}_{0}+\\hat{\\beta}_{1} x_{i}$为样本在自变量为$x_i$下的拟合值，记样本实际观测值$y_i$与拟合值$\\hat{y}_{i}$之差为拟合残差$\n",
    "\\hat{u}_{i}=y_{i}-\\hat{y}_{i}$。\n",
    "\n",
    "不同的距离定义方法是不同估计法的一大区别，OLS对距离的定义是：残差的平方${\\hat{u}_i}^2$。因此OLS估计的思想是：**OLS估计求得的系数$\\hat{\\beta}_{0}$、$\\hat{\\beta}_{1}$，将使直线与所有样本的拟合残差的平方和最小**，即\n",
    "$$\n",
    "\\left(\\hat{\\beta}_{0}, \\hat{\\beta}_{1}\\right)=\\operatorname{argmin} \\sum_{i=1}^{n}\\left(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1} x_{i}\\right)^{2}\n",
    "$$\n",
    "对于多元线性回归，OLS估计的思想也完全相同，只不过多元线性回归的模型不是一条直线，而是一个多维的超平面。对于多元线性回归的OLS估计目标函数，有\n",
    "$$\n",
    "\\left( \\hat{\\beta}_0,\\cdots ,\\hat{\\beta}_k \\right) =\\mathrm{arg}\\min \\sum_{i=1}^n{\\left( y_i-\\hat{\\beta}_0-\\hat{\\beta}_1x_{1i}-\\hat{\\beta}_kx_{ki} \\right) ^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 OLS估计的求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 从优化角度看OLS求解**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在上面知晓了需要求解的函数后，接下来就要开始进行求解了。\n",
    "\n",
    "记目标函数为\n",
    "$$\n",
    "Q\\left(\\hat{\\beta}_{0}, \\hat{\\beta}_{1}, \\cdots, \\hat{\\beta}_{k}\\right)=\\sum_{i=1}^{n}\\left(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1} x_{i 1}-\\cdots-\\hat{\\beta}_{k} x_{i k}\\right)^{2}\n",
    "$$\n",
    "这是一个以$(\\hat{\\beta}_{0}, \\hat{\\beta}_{1}, \\cdots, \\hat{\\beta}_{k})$作为未知变量的多元函数，我们要求得最小值点，可以令各元偏导数等于0，构建一个$k+1$维的方程组求解：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\sum_{i=1}^{n}\\left(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1} x_{i 1}-\\cdots-\\hat{\\beta}_{k} x_{i k}\\right)=0 \\\\\n",
    "&\\sum_{i=1}^{n}\\left(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1} x_{i 1}-\\cdots-\\hat{\\beta}_{k} x_{i k}\\right) x_{i 1}=0 \\\\\n",
    "&\\sum_{i=1}^{n}\\left(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1} x_{i 1}-\\cdots-\\hat{\\beta}_{k} x_{i k}\\right) x_{i 2}=0 \\\\\n",
    "&\\cdots \\quad \\cdots \\\\\n",
    "&\\sum_{i=1}^{n}\\left(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1} x_{i 1}-\\cdots-\\hat{\\beta}_{k} x_{i k}\\right) x_{i k}=0\n",
    "\\end{aligned}\n",
    "$$\n",
    "以上方程组中，每个方程有$k+1$个自变量，且有$k+1$个方程，根据线性代数的知识，我们可以求得$(\\hat{\\beta}_{0}, \\hat{\\beta}_{1}, \\cdots, \\hat{\\beta}_{k})$的唯一解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· OLS求解的矩阵表示**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述思路证明了OLS估计求解系数的可行性，但是我们还不知道系数估计的具体形式，接下来我们就利用矩阵的形式给出OLS求解的结果。求解过程无需掌握，但大家需要大致了解每个数学符号的含义与指代，在后续的理论介绍时，它们会被反复提及。\n",
    "\n",
    "由于我们有$n$个样本，因此根据模型有以下$n$个等式成立\n",
    "$$\n",
    "y_{i}=\\beta_{0}+\\beta_{1} x_{i 1}+\\cdots+\\beta_{k} x_{i k}+u_{i}, \\quad i=1, \\cdots, n\n",
    "$$\n",
    "将它们联立称方程组，并表示成矩阵形式\n",
    "$$\n",
    "\\boldsymbol{y}=\\boldsymbol{X\\beta }+\\boldsymbol{u}\n",
    "$$\n",
    "这里，$\\boldsymbol{y}=\\left( y_1,y_2,\\cdots ,y_n \\right) ^{'},\\quad \\boldsymbol{\\beta} =\\left( \\beta _0,\\beta _1,\\cdots ,\\beta _k \\right) ^{'},\\quad \\boldsymbol{u}=\\left( u_1,u_2,\\cdots ,u_n \\right) ^{'}$。\n",
    "<br>\n",
    "并记：$x_{i}^{\\prime}=\\left(1, x_{i 1}, x_{i 2}, \\cdots, x_{i k}\\right), \\boldsymbol{X}=\\left(x_{1}^{\\prime}, x_{2}^{\\prime}, \\cdots, x_{n}^{\\prime}\\right)^{\\prime}$，值得注意的是，$\\boldsymbol{X}$是一个$n\\times \\left( k+1 \\right) $维的矩阵，n为样本个数，k为自变量个数，它也被称为设计阵。\n",
    "\n",
    "以上是真实模型的矩阵表示形式，对于我们实际拟合的模型及其残差，其矩阵形式则为\n",
    "$$\n",
    "\\boldsymbol{\\hat{y}}=\\boldsymbol{X\\hat{\\beta}},\\quad \\boldsymbol{\\hat{u}}=\\boldsymbol{y}-\\boldsymbol{\\hat{y}}\n",
    "$$\n",
    "根据令残差平方和偏导数为0的思想，有\n",
    "$$\n",
    "Q(\\hat{\\beta})=\\sum_{i=1}^{n} \\hat{u}_{i}^{2}=\\hat{u}^{\\prime} \\hat{u}=(y-X \\hat{\\beta})^{\\prime}(y-X \\hat{\\beta})=y^{\\prime} y-2 \\hat{\\beta}^{\\prime} X^{\\prime} y+\\hat{\\beta}^{\\prime} X^{\\prime} X \\hat{\\beta}\n",
    "$$\n",
    "运用向量求导的知识得\n",
    "$$\n",
    "X^{\\prime} X \\hat{\\beta}=X^{\\prime} y \\Rightarrow \\hat{\\beta}=\\left(X^{\\prime} X\\right)^{-1} X^{\\prime} y\n",
    "$$\n",
    "至此，我们就得到了各系数估计向量$\\hat{\\beta}$的矩阵表达式了。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "手动计算的系数向量为：\n",
      "[1.28632777 0.00942601 0.45345589]\n",
      "-----------------------------------\n",
      "软件计算的系数为：\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>1.286328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACT</th>\n",
       "      <td>0.009426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hsGPA</th>\n",
       "      <td>0.453456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         params\n",
       "const  1.286328\n",
       "ACT    0.009426\n",
       "hsGPA  0.453456"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 以gpa_lm2为例\n",
    "# 手动计算系数的估计向量\n",
    "X2_T = X2.values.T\n",
    "X_inv = np.linalg.inv(np.dot(X2_T, X2))  # 求矩阵乘积的逆矩阵\n",
    "Xy = np.dot(X2_T, y.values)\n",
    "beta_hat = np.dot(X_inv, Xy)\n",
    "print('手动计算的系数向量为：')\n",
    "print(beta_hat)\n",
    "\n",
    "# 软件计算的系数向量\n",
    "print('-----------------------------------')\n",
    "print('软件计算的系数为：')\n",
    "display(c2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 拟合优度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于多元线性模型\n",
    "$$\n",
    "y=\\beta_{0}+\\beta_{1} x_{1}+\\cdots+\\beta_{k} x_{k}+u\n",
    "$$\n",
    "我们使用OLS得到了一个拟合模型\n",
    "$$\n",
    "\\hat{y}=\\hat{\\beta}_0+\\hat{\\beta}_1x_1+\\cdots +\\hat{\\beta}_kx_k\n",
    "$$\n",
    "一个很自然的问题是：这个模型对数据的拟合效果如何？这个问题可以进一步引申为：模型中的自变量$x_i$在多大程度上解释了$y$的变异？（$y$的趋势变化可以理解为是一种带有规律性的变异）\n",
    "\n",
    "在探讨这个问题前，我们先引入几个简单而又重要的概念。\n",
    "\n",
    "· TSS(Total sum of squares)，总平方和\n",
    "$$\n",
    "T S S=\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}\n",
    "$$\n",
    "\n",
    "· ESS(Explained sum of squares)，解释平方和\n",
    "$$\n",
    "E S S=\\sum_{i=1}^{n}\\left(\\hat{y}_{i}-\\bar{y}\\right)^{2}\n",
    "$$\n",
    "\n",
    "· RSS(Resiual sum of squares)，残差平方和\n",
    "$$\n",
    "R S S=\\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}\n",
    "$$\n",
    "直观上RSS是一个可以度量拟合优度的量，因为残差平方和越小，意味着预测值与真实值之间的差距越小。但是RSS的大小没有一个标准，它将随着样本量的增大而增大，因此单纯的RSS不是一个合格的衡量拟合优度的量。\n",
    "\n",
    "这个时候我们可以从另一个角度去理解回归建模的意义。我们之所以想构建模型，是因为想找到**造成$y$值变化**的因素，模型解释的变异占总变异的比例越多，这个模型的解释力度就越大，模型的拟合优度也就越好。我们举一个简单的例子：某天，一个村子的菜包子涨了1块钱，大家都想知道究竟是什么原因导致这1块钱的涨幅。小红和小明综合了当天所有发生变化的外因素（其实就是自变量啦~），分别构建了两个模型将这些外因素的变化和菜包子涨价的1块钱联系在一起。在小红的模型预测下，这些外因素变化会使菜包子涨价0.99块钱，而小明的模型则只预测到了0.1块钱的涨价。我们认为，小红的模型解释1块钱涨价中的0.99块，而小明只解释了0.1块，因此小红的模型更优。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 回归拟合优度——R方**\n",
    "\n",
    "理解了用“解释变异的比例”衡量回归模型拟合优度的思想，构造拟合优度就有思路了。回归分析中最常用的拟合优度是R方，定义为\n",
    "$$\n",
    "R^{2}=\\frac{E S S}{T S S}\n",
    "$$\n",
    "其中，TSS度量了因变量$y$的总样本变异，而ESS度量了模型拟合值$\\hat{y}$的总变异，也就是解释了的变异。事实上三种平方和存在关系$TSS=RSS+ESS$（大家可以尝试自己推导），这说明：总变异可以被拆分为解释了变异和未被解释的变异，残差平方和度量了“剩余信息”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "手动计算的R方为：0.1764216724583183\n",
      "-------------------------------------------------\n",
      "软件计算的R方为：0.17642159463450557\n"
     ]
    }
   ],
   "source": [
    "# 动手计算模型gpa_lm2的R方\n",
    "TSS_gpa=np.sum(np.power(gpa1.colGPA-np.mean(gpa1.colGPA),2))\n",
    "RSS_gpa=np.sum(np.power(gpa_lm2.resid,2))\n",
    "gpa_lm2_R2=1-RSS_gpa/TSS_gpa\n",
    "print('手动计算的R方为：{}'.format(gpa_lm2_R2))\n",
    "print('-------------------------------------------------')\n",
    "# 直接输出模型gpa_lm2的R方\n",
    "gpa_lm2_R2=gpa_lm2.rsquared\n",
    "print('软件计算的R方为：{}'.format(gpa_lm2_R2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两者结果十分接近，之所以不完全相同可能是numpy计算与statsmodels计算存在小差异。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.推断和假设检验\n",
    "\n",
    "我们用一个例子引出回归分析中推断任务的意义。\n",
    "\n",
    "**· Example3.** 依旧以上面的大学成绩例子为例。我们有一个直觉：成绩较好的学生似乎更倾向于不旷课，因此我们想知道旷课究竟会不会影响考试成绩。由于考试成绩取决于很多因素，我们要评价旷课(skipped)对成绩的影响，就需要建立一个多元线性回归模型，以控制其他因素的影响。通过OLS估计，方程结果如下\n",
    "$$\n",
    "\\mathrm{colGPA}=1.390+0.412\\mathrm{hsGPA}+0.015\\mathrm{ACT}-0.083\\mathrm{skipped}+u\n",
    "$$\n",
    "接下来的问题是，旷课skipped这一因素怎样子才能算是影响考试成绩呢？显然，如果它的系数非常接近0，那么它对成绩的影响是不明显的，换言之，是**不显著的**。事实上，我们后续在回归分析中所提及的“系数显著性”，本质上都是“**系数不为0的显著性**”。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 t检验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t检验是回归分析中单个线性假设检验问题的常用检验方法。单个线性假设检验问题可分为如下：\n",
    "\n",
    "· **单参数检验问题**：$H_{0}: \\beta_{j}=\\beta_{j 0} \\leftrightarrow H_{1}: \\beta_{j} \\neq \\beta_{j 0}$（$\\beta_{j 0}$为任意常数）\n",
    "\n",
    "这类问题的典型问题就是系数的显著性检验$H_{0}: \\beta_{j}=0 \\leftrightarrow H_{1}: \\beta_{j} \\neq 0$\n",
    "\n",
    "· **参数线性组合检验问题**：$H_0:f\\left( \\beta \\right) =\\beta _0\\leftrightarrow H_1:f\\left( \\beta \\right) \\ne \\beta _0$（$\\beta_{0}$为任意常数）\n",
    "\n",
    "这类问题的典型问题就是系数间的相等性检验$H_0:\\beta _i=\\beta _j\\leftrightarrow H_1:\\beta _i\\ne \\beta _j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 t检验的思想-从单参数检验说起"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有关假设检验严格的理论推导，大家可参考概率论与数理统计中的假设检验部分，在此我们只对其思想进行简单的介绍。\n",
    "\n",
    "正如上面介绍的，回归系数的显著性检验就是“系数是否为0”的检验\n",
    "$$\n",
    "H_{0}: \\beta_{j}=0 \\leftrightarrow H_{1}: \\beta_{j} \\neq 0\n",
    "$$\n",
    "如何根据样本数据对这一问题作出结论呢？我们的想法是，既然$\\hat{\\beta_{j}}$是$\\beta_{j}$的无偏估计，那么如果原假设成立，即真的有$\\beta_{j}=0$，那么$\\hat{\\beta_{j}}$有很大的可能性位于0附近；相反，如果实际样本计算出的$\\hat{\\beta_{j}}$远离0，那么这个假设**有很大可能不成立**。为了有一个确定的答案，我们设立一个临界值$C$，若$\\left|\\hat{\\beta}_{j}-0\\right|>C$，我们就拒绝假设$H_0$\n",
    "\n",
    "**· 临界值与置信水平**\n",
    "\n",
    "接下来的问题是，如何确定$C$呢？用概率。\n",
    "\n",
    "由于抽样的随机性，我们根据$\\hat{\\beta_{j}}$判断$\\beta_{j}$的命题，不论拒绝与否，都**有概率**会犯以下两类错误的其中之一：\n",
    "\n",
    "· 第一类错误，即原假设成立但是我们拒绝了它。犯第一类错误的概率称为拒真概率。\n",
    "\n",
    "· 第二类错误，即原假设不成立但是我们没有拒绝它。\n",
    "\n",
    "我们定夺临界值的时候，要保证发生第一类错误的概率需要在一个给定的、较小的水平$\\alpha$，这个$\\alpha$也被称为显著性水平，1 - $\\alpha$为置信水平。如此以来，我们考虑临界值$C$的判准是，原假设$H_{0}$成立但是$\\left|\\hat{\\beta}_{j}-\\hat{\\beta}_{j0}\\right|>C$（因而拒绝原假设$H_{0}$）的概率应当恰好为我们人为给定的$\\alpha$，即\n",
    "$$\n",
    "P_{H_0\\,\\,is\\,\\,true}\\left( \\left| \\hat{\\beta}_j-\\beta _{j0} \\right|>C \\right) =P\\left( \\left| \\hat{\\beta}_j-0 \\right|>C \\right) =\\alpha \n",
    "$$\n",
    "\n",
    "**· 用t分布处理概率**\n",
    "\n",
    "现在我们就要开始处理$P\\left(\\left|\\hat{\\beta}_{j}-0\\right|>C\\right)$了。\n",
    "\n",
    "在前面的OLS估计的正态分布性质中我们得知$\\frac{\\hat{\\beta}_{j}-\\beta_{j}}{\\operatorname{se}\\left(\\hat{\\beta}_{j}\\right)} \\sim t_{n-k-1}$，而在原假设$H_{0}$成立时$\\beta_j=0$，因此$\\frac{\\hat{\\beta}_j}{\\mathrm{se}\\left( \\hat{\\beta}_j \\right)}\\sim t_{n-k-1}$，我们便可以使用t分布处理以上概率\n",
    "$$\n",
    "P\\left(\\left|\\hat{\\beta}_{j}\\right|>C\\right)=P\\left(\\frac{\\left|\\hat{\\beta}_{j}\\right|}{\\operatorname{se}\\left(\\hat{\\beta}_{j}\\right)}>\\frac{C}{\\operatorname{se}\\left(\\hat{\\beta}_{j}\\right)}\\right)=\\alpha\n",
    "$$\n",
    "由于$\\frac{\\hat{\\beta}_j}{\\mathrm{se}\\left( \\hat{\\beta}_j \\right)}$服从自由度为$n-k-1$的t分布，因此要让概率为$\\alpha$，$\\frac{C}{\\mathrm{se}\\left( \\hat{\\beta}_j \\right)}$应等于$1-\\frac{\\alpha}{2}$分位点，记为$t_{n-k-1}\\left( 1-\\frac{\\alpha}{2} \\right) $\n",
    "\n",
    "于是，$C=t_{n-k-1}(1-\\alpha /2)\\mathrm{se}\\left( \\hat{\\beta}_j \\right) $，如果我们计算出来的$\\hat{\\beta_j}$有：$\\left| \\hat{\\beta}_j \\right|>t_{n-k-1}(1-\\alpha /2)\\mathrm{se}\\left( \\hat{\\beta}_j \\right) $，那么我们便可以拒绝原假设，这个系数是显著的！\n",
    "\n",
    "当然，在python实现的时候，我们不会直接比较临界值$C$与$|\\hat{\\beta}_j|$(因为计算$C$很麻烦)，而是先计算$\\frac{\\hat{\\beta}_{j}-\\beta_{j}}{\\operatorname{se}\\left(\\hat{\\beta}_{j}\\right)}$，再与python输出的t分布对应的双侧分位点值$\\pm {t_{n-k-1}\\left( 1-\\frac{\\alpha}{2} \\right)} $进行比较。\n",
    "\n",
    "接下来，我们先进行手动假设检验，检验的问题为：\n",
    "$$\n",
    "H_{0}: \\beta_{3}=0 \\leftrightarrow H_{1}: \\beta_{3} \\neq 0\n",
    "$$\n",
    "置信水平为0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "双侧分位点为：(-1.9774312122928936, 1.9774312122928936)\n",
      "t值为：-3.1968396347468278\n",
      "t值小于左侧分位点，位于拒绝域，因此在0.05的显著性水平可以拒绝原假设，即skipped系数不为0.\n"
     ]
    }
   ],
   "source": [
    "# 手动进行假设检验\n",
    "from scipy.stats import t\n",
    "gpa_lm3 = sm.formula.ols('colGPA~hsGPA+ACT+skipped', data=gpa1).fit()\n",
    "\n",
    "# 计算t值\n",
    "skipped = gpa_lm3.params[3]\n",
    "se_skipped = gpa_lm3.bse[3]\n",
    "tvalue = skipped/se_skipped\n",
    "\n",
    "# 计算分位点\n",
    "'''\n",
    "ppf:单侧左分位点\n",
    "isf:单侧右分位点\n",
    "interval:双侧分位点\n",
    "'''\n",
    "T_int = t.interval(\n",
    "    0.95, gpa_lm3.df_resid)  # 对于双侧检验（双侧分位点），分位点参数应该输入1-a，这里是1-0.05=0.95\n",
    "print('双侧分位点为：{}'.format(T_int))\n",
    "print('t值为：{}'.format(tvalue))\n",
    "print('t值小于左侧分位点，位于拒绝域，因此在0.05的显著性水平可以拒绝原假设，即skipped系数不为0.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，我们检验的问题还可以变为\n",
    "$$\n",
    "H_{0}: \\beta_{3}=-0.1 \\leftrightarrow H_{1}: \\beta_{3} \\neq -0.1\n",
    "$$\n",
    "我们只需要变更t值而不需要变更t分位点值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t值为：0.6495314591856176\n",
      "此时t值小于右侧分位点但大于左侧分位点，位于接受域，不能拒绝原假设，即skipped系数可为-0.1\n"
     ]
    }
   ],
   "source": [
    "tvalue=(skipped+0.1)/se_skipped\n",
    "print('t值为：{}'.format(tvalue))\n",
    "print('此时t值小于右侧分位点但大于左侧分位点，位于接受域，不能拒绝原假设，即skipped系数可为-0.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 单边检验——换汤不换药**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面我们介绍的是t检验的双边检验，它的假设是这样的\n",
    "$$H_{0}: \\beta_{j}=\\beta_{j 0} \\leftrightarrow H_{1}: \\beta_{j} \\neq \\beta_{j 0}$$\n",
    "双边检验回答的问题是：实际参数是否“靠近”我们假设的值。而有时候我们回归分析中可能还会有这样的问题：某某自变量对因变量是否存在正效应影响呢？这个问题其实等价于下面的假设\n",
    "$$\n",
    "H_0:\\beta _j=\\beta _{j0}\\leftrightarrow H_1:\\beta _j>\\beta _{j0}\\,\\,\\left( \\beta _{j0}=0 \\right) \n",
    "$$\n",
    "单边检验的分析思路和双边检验基本一样，只不过$P\\left(\\left|\\hat{\\beta}_{j}-0\\right|>C\\right)$要变为$P\\left( \\hat{\\beta}_j-0>C \\right) $，$\\frac{C}{\\operatorname{se}\\left(\\hat{\\beta}_{j}\\right)}$也应从$1-\\frac{\\alpha}{2}$分位点变为$1-\\alpha $分位点(大家可以思考一下为什么)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们依旧进行手动检验，检验问题为\n",
    "$$\n",
    "H_0:\\beta _j=0\\leftrightarrow H_1:\\beta _j<0\n",
    "$$\n",
    "置信水平为0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "左侧分位点为：-1.6560520804924401\n",
      "t值为：-3.1968396347468304\n",
      "t值小于左侧分位点，位于拒绝域，因此在0.05的显著性水平可以拒绝原假设，即skipped系数小于0.\n"
     ]
    }
   ],
   "source": [
    "tvalue=skipped/se_skipped\n",
    "# 因为是小于，因此看左分位点\n",
    "T_right=t.ppf(0.05,gpa_lm3.df_resid) # 对于单侧检验，分位点参数应该输入a，这里是0.05\n",
    "print('左侧分位点为：{}'.format(T_right))\n",
    "print('t值为：{}'.format(tvalue))\n",
    "print('t值小于左侧分位点，位于拒绝域，因此在0.05的显著性水平可以拒绝原假设，即skipped系数小于0.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· p值**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用临界值$C$与$\\left|\\hat{\\beta}_{j}-\\hat{\\beta}_{j0}\\right|$作比较有一个缺点，就是分位点值与置信水平$\\alpha$相关的。如果我们要在不同的置信水平下检验，就需要计算不同的分位点再比较，这样很繁琐。这个时候，我们可以使用p值。\n",
    "\n",
    "p值是在本次分析的样本观测值下，给出的能拒绝原假设的最小置信水平，它只与样本观测值和我们做的假设检验有关。p值越小越可以拒绝原假设，例如：如果p值为0.001，比0.01的置信水平还要小，我们认为在0.01的置信水平下我们也可以拒绝原假设；而如果p值为0.025，比0.01的置信水平要大，但小于0.05，则我们认为在0.05的置信水平下我们可以拒绝原假设，但在0.01置信水平下不可以拒绝。\n",
    "\n",
    "p值的形式与我们做的备择假设$H_1$有关：\n",
    "\n",
    "· 若$H_{1}: \\beta_{j} \\neq \\beta_{j 0}$，则：$pvalue=P\\left( \\left| t_{n-k-1} \\right|>\\left| \\frac{\\hat{\\beta}_j-\\beta _{j0}}{se\\left( \\hat{\\beta}_j \\right)} \\right| \\right) $\n",
    "\n",
    "· 若$H_{1}: \\beta_{j} > \\beta_{j 0}$，则：$pvalue=P\\left( t_{n-k-1}>\\frac{\\hat{\\beta}_j-\\beta _{j0}}{se\\left( \\hat{\\beta}_j \\right)} \\right) $\n",
    "\n",
    "· 若$H_{1}: \\beta_{j} < \\beta_{j 0}$，则：$pvalue=P\\left( t_{n-k-1}<\\frac{\\hat{\\beta}_j-\\beta _{j0}}{se\\left( \\hat{\\beta}_j \\right)} \\right) $\n",
    "\n",
    "可以看到，p值本质上是一种累积概率，且对于同一个$\\beta_{j 0}$而言，双边检验的p值为单边检验的两倍（在代码实现中我们可以看到这点）。我们先利用p值手动检验以下问题\n",
    "$$\n",
    "H_{0}: \\beta_{3}=0 \\leftrightarrow H_{1}: \\beta_{3} \\neq 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设检验的思想是小概率事件。事先提出一个假设（原假设），通过数据采样的结果计算检验统计量，这些检验统计量在原假设的条件有对应的抽样分布，若这个检验统计量（包含更极端的情况）在原假设条件下发生的概率很小，那么就有理由拒绝原假设。这个很小的概率对应的就是P值，而alpha则是可容忍的最小概率（显著性水平）。\n",
    "\n",
    "关于p值的设定，是根据我们事先的检验水准a确定的，a一般取0.05，在统计学上一般认为p小于0.05是不可能事件即在单次试验中不可能发生\n",
    "\n",
    "alpha最简单的解释就是一类错误的概率。\n",
    "\n",
    "P就是根据现在的数据能得到最小的一类错误的概率，如果你假设的alpha=0.05小于p那就说明目前数据还不能满足你的假设需求，不能拒绝原假设"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题1: 备择假设是什么? 当t值和备择假设相反时，p值要按备择假设来定吗?\n",
    "\n",
    "> 备则假设是自己确定的，可以是不等于，小于，大于。summary中的是不等于\n",
    ">\n",
    "> p值的计算方式。备则假设大于，不等于，小于0，都有相应的p值计算方法\n",
    ">\n",
    "> 只不过，summary中的p值是备则假设不等于0这种情况，而备则假设不等于0的p值，要么是小于0的p值的两倍，要么是大于0的p值的两倍。\n",
    ">\n",
    "> 所以我们得判断是哪种情况? 判断的方法是，看t值，如果t值小于0，那么软件包里的p值就是小于0的备则假设的p值的两倍；大于0相反"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题2: 小于0的假设是在哪里做出的，H0不是假设参数等于0吗?\n",
    "\n",
    "> 假如备则假设是小于零，那么原假设设置为大于等于0或等于0，这两个假设是一样的\n",
    ">\n",
    "> 我们在回归中做假设检验一般都是根据系数的估计值做假设的，比如系数是-0.03，你猜想系数的真值可能小于0，于是设置备则假设为小于0。此时，如果我们可以拒绝等于0的，那就一定可以拒绝大于0\n",
    ">\n",
    "> 所以在原假设中设置大于等于0或者等于0，这两者都是一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "由于双边检验p值是对单边检验p值乘两倍得来的，我们要根据t值是否大于0来选择左/右尾累积概率，若小于0，则选择左尾；反之右尾。\n",
      "True\n",
      "p值为：0.002\n",
      "p值非常小，可见我们可以拒绝原假设\n"
     ]
    }
   ],
   "source": [
    "# 计算t值仍然是第一步\n",
    "tvalue=skipped/se_skipped\n",
    "print('由于双边检验p值是对单边检验p值乘两倍得来的，我们要根据t值是否大于0来选择左/右尾累积概率，若小于0，则选择左尾；反之右尾。')\n",
    "'''\n",
    "sf:右尾累积概率\n",
    "cdf:左尾累积概率\n",
    "'''\n",
    "print(tvalue<0)\n",
    "pvalue=t.cdf(tvalue,gpa_lm3.df_resid)*2 # 双边p值记得乘2\n",
    "print('p值为：{:.3f}'.format(pvalue)) # 保留三位小数\n",
    "print('p值非常小，可见我们可以拒绝原假设')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事实上，summary中的p值，正是系数0值双边检验的p值，我们查看一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 colGPA   R-squared:                       0.234\n",
      "Model:                            OLS   Adj. R-squared:                  0.217\n",
      "Method:                 Least Squares   F-statistic:                     13.92\n",
      "Date:                Mon, 04 Jul 2022   Prob (F-statistic):           5.65e-08\n",
      "Time:                        21:14:49   Log-Likelihood:                -41.501\n",
      "No. Observations:                 141   AIC:                             91.00\n",
      "Df Residuals:                     137   BIC:                             102.8\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      1.3896      0.332      4.191      0.000       0.734       2.045\n",
      "hsGPA          0.4118      0.094      4.396      0.000       0.227       0.597\n",
      "ACT            0.0147      0.011      1.393      0.166      -0.006       0.036\n",
      "skipped       -0.0831      0.026     -3.197      0.002      -0.135      -0.032\n",
      "==============================================================================\n",
      "Omnibus:                        1.917   Durbin-Watson:                   1.881\n",
      "Prob(Omnibus):                  0.383   Jarque-Bera (JB):                1.636\n",
      "Skew:                           0.125   Prob(JB):                        0.441\n",
      "Kurtosis:                       2.535   Cond. No.                         300.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(gpa_lm3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，对于系数的非0值单边检验，我们也可以进行手动检验，考虑下面问题\n",
    "$$\n",
    "H_{0}: \\beta_{3}=-0.1 \\leftrightarrow H_{1}: \\beta_{3} > -0.1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p值为：0.259\n",
      "p值远大于0.1，可见我们不能拒绝原假设\n"
     ]
    }
   ],
   "source": [
    "# 还是先计算t值！\n",
    "tvalue=(skipped+0.1)/se_skipped\n",
    "pvalue=t.sf(tvalue,gpa_lm3.df_resid) # 由于备择假设是大于号，因此要用右尾累积概率，且不用乘2\n",
    "print('p值为：{:.3f}'.format(pvalue)) # 保留三位小数\n",
    "print('p值远大于0.1，可见我们不能拒绝原假设')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.自变量的多种形式\n",
    "\n",
    "在前面的章节，我们学习了最经典、最简单的多元线性回归模型\n",
    "$$\n",
    "y=\\beta_{0}+\\beta_{1} x_{1}+\\beta_{2} x_{2}+\\cdots+\\beta_{k} x_{k}+u\n",
    "$$\n",
    "为了大家在学习之初不被混淆，我们其实默认了自变量都是一次项的、定量的变量。实际上，自变量不仅可以是一次的连续变量，还可以是一种**定性变量**，也可以是某个**变量的函数**，如二次项$X^2$、对数项$log(X)$。这是因为，所谓的线性回归模型，线性关系并不是指代被解释变量$y$与解释变量$X$之间的关系，而是指回归函数相对于**回归系数**是线性的。\n",
    "\n",
    "在这一章节，我们将重点学习带有定性变量的回归，并简单介绍常用的带有变量函数的回归（如带有对数项）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 带有定性变量的回归分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前我们所考虑的模型里的变量都是定量变量，如：工资，产品销售量等，其取值有大小的区分。而在实际问题中，还有一些诸如性别、种族、季节、婚姻状态等定性变量，也称为类别变量。我们只讨论自变量带有定性变量的情况，不讨论因变量是定性变量的情况，因为此时问题将变为分类问题而非回归问题。\n",
    "\n",
    "我们先讨论最简单的二分类变量，再讨论多分类变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 二分类变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 定性变量定量化——虚拟变量/哑变量**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定性变量的取值一般都不是数字，如性别变量的取值为男/女、婚姻状态的取值为是/否，\n",
    "\n",
    "最方便也是最高效的定量化就是用0-1变量定义二分类变量。一般而言，0表示“否”，1表示“是”，对于性别$sex$这个变量，定义$sex=0$为男士，$sex=1$为女士\n",
    "\n",
    "这种替代定性变量性别的$sex$变量被称为**虚拟变量/哑变量(Dummy Variable)**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 多分类变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相比于二分类变量，多分类变量在现实中更常见。如季节变量（春、夏、秋、冬）；地理位置变量（华南、华北、华东、中部、西部）等。\n",
    "\n",
    "与此同时，多分类变量相比于二分类变量也更难处理，我们不能用一个变量的多个取值来定义多分类虚拟变量。如，我们不可以定义季节变量$season$的取值1/2/3/4为春/夏/秋/冬，因为这意味着不同分类之间的差异完全取决于取值之间的差！\n",
    "\n",
    "正确做法是，用多个二值虚拟变量来表示多分类定性变量。**具体的，如果一个变量有n个类别，则需要定义n-1个虚拟变量表示它**。以季节变量为例，我们定义三个虚拟变量：spring/summer/fall，当它们其中之一等于1时，代表季节为它们本身；而如果它们全都为0，则代表季节为winter。\n",
    "\n",
    "**· 虚拟变量陷阱——完全共线性**\n",
    "\n",
    "之所以需要这样定义多分类定性变量，是因为如果我们如果将winter也纳入模型中时，这四个变量会满足一个恒等关系式\n",
    "$$\n",
    "spring+summer+fall+winter=1\n",
    "$$\n",
    "这说明这四个自变量存在完全共线性，违背了CLM假设中的MLR.4，使得模型完全失效。\n",
    "\n",
    "接下来，我们对一个含有多分类变量的实例使用python进行回归，在本次python实现中你将会学习：\n",
    "\n",
    "1. 如何将一个多分类变量“分解”为多个0-1虚拟变量\n",
    "2. 分解成多个虚拟0-1变量后，如何用这些变量进行回归而不踩雷。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· Example9.** 我们对某大公司的计算机专业人员进行薪水调查，调查的目的是识别和量化哪些影响薪水差异的因素，数据中的变量描述如下：\n",
    "\n",
    "S：年薪，单位是美元；\n",
    "<br>\n",
    "X：工作经验，单位是年；\n",
    "<br>\n",
    "E：教育，1表示高中毕业，2表示获得学士学位，3表示更高学位；\n",
    "<br>\n",
    "M：1表示为管理人员，0表示非管理人员；\n",
    "\n",
    "我们以S为因变量，以X/E/M为自变量进行多元回归。其中：X为定量变量，M为二分类变量（且已经0-1化），它们已经可以直接进行回归处理了。但是E则需要进行0-1处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S</th>\n",
       "      <th>X</th>\n",
       "      <th>E</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13876</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11608</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18701</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11283</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11767</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       S  X  E  M\n",
       "0  13876  1  1  1\n",
       "1  11608  1  3  0\n",
       "2  18701  1  3  1\n",
       "3  11283  1  2  0\n",
       "4  11767  1  3  0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_table('./data/P130.txt')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们对E进行虚拟变量编码，这也叫One-hot编码(独热编码)。我们使用pandas包的get_dummies函数进行重编码。\n",
    "\n",
    "在对E进行重编码前，我们先花一点时间介绍一下get_dummies这一函数，让大家明白这个函数的工作原理。\n",
    "\n",
    "该函数会自动变换所有具有对象类型（如字符串）的列，但是如果某列的变量是数值型变量（哪怕它实际上是分类变量），它将不会为该列创建虚拟变量，除非我们将该列的数据类型从数值转化为字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Interger_Feature</th>\n",
       "      <th>Categorical_Feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>socks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>fox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>socks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>box</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Interger_Feature Categorical_Feature\n",
       "0                 0               socks\n",
       "1                 1                 fox\n",
       "2                 2               socks\n",
       "3                 1                 box"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 创建一个dataframe，它包含一个整数特征与分类字符串特征\n",
    "demo_df=pd.DataFrame({'Interger_Feature':[0,1,2,1],'Categorical_Feature':['socks','fox','socks','box']})\n",
    "display(demo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Interger_Feature</th>\n",
       "      <th>Categorical_Feature_box</th>\n",
       "      <th>Categorical_Feature_fox</th>\n",
       "      <th>Categorical_Feature_socks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Interger_Feature  Categorical_Feature_box  Categorical_Feature_fox  \\\n",
       "0                 0                        0                        0   \n",
       "1                 1                        0                        1   \n",
       "2                 2                        0                        0   \n",
       "3                 1                        1                        0   \n",
       "\n",
       "   Categorical_Feature_socks  \n",
       "0                          1  \n",
       "1                          0  \n",
       "2                          1  \n",
       "3                          0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "# 使用get_dummies函数\n",
    "display(pd.get_dummies(demo_df))\n",
    "print(type(demo_df.Interger_Feature[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到，经过变换后，Interger Feature列由于取值的数值类型是int64（数值型的一种），因此它没有被“分解”为多个虚拟变量。如果我们想对其进行编码，需要将它的变量类型转化为字符串str。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categorical_Feature</th>\n",
       "      <th>Interger_Feature_0</th>\n",
       "      <th>Interger_Feature_1</th>\n",
       "      <th>Interger_Feature_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>socks</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fox</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>socks</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>box</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Categorical_Feature  Interger_Feature_0  Interger_Feature_1  \\\n",
       "0               socks                   1                   0   \n",
       "1                 fox                   0                   1   \n",
       "2               socks                   0                   0   \n",
       "3                 box                   0                   1   \n",
       "\n",
       "   Interger_Feature_2  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   1  \n",
       "3                   0  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_df['Interger_Feature']=demo_df['Interger_Feature'].astype(str)\n",
    "\n",
    "pd.get_dummies(demo_df,columns=['Interger_Feature']) # 指定columns参数，就可以对我们想要虚拟变量化的列进行精准转换\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_dummies函数的基本使用方法已经介绍完毕，我们开始对E进行重编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S</th>\n",
       "      <th>X</th>\n",
       "      <th>M</th>\n",
       "      <th>E_1</th>\n",
       "      <th>E_2</th>\n",
       "      <th>E_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13876</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11608</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18701</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11283</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11767</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       S  X  M  E_1  E_2  E_3\n",
       "0  13876  1  1    1    0    0\n",
       "1  11608  1  0    0    0    1\n",
       "2  18701  1  1    0    0    1\n",
       "3  11283  1  0    0    1    0\n",
       "4  11767  1  0    0    0    1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['E']=data['E'].astype(str)\n",
    "data_dummies=pd.get_dummies(data,columns=['E'])\n",
    "data_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时，我们已经将变量E转化为了三个虚拟变量，现在可以进行回归了。注意：不可以直接将三个虚拟变量同时纳入回归当中，我们可以选取一个虚拟变量作为“基组”，然后将其他非基组的虚拟变量纳入回归。这里我们选取高中教育水平$E=1$作为基组，则方程可以这样构建\n",
    "$$\n",
    "S=\\beta_{0}+\\beta_{1} X+\\gamma_{2} E_{2}+\\gamma_{3} E_{3}+\\delta_{1} M+u\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      S   R-squared:                       0.957\n",
      "Model:                            OLS   Adj. R-squared:                  0.953\n",
      "Method:                 Least Squares   F-statistic:                     226.8\n",
      "Date:                Mon, 04 Jul 2022   Prob (F-statistic):           2.23e-27\n",
      "Time:                        22:42:02   Log-Likelihood:                -381.63\n",
      "No. Observations:                  46   AIC:                             773.3\n",
      "Df Residuals:                      41   BIC:                             782.4\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   8035.5976    386.689     20.781      0.000    7254.663    8816.532\n",
      "X            546.1840     30.519     17.896      0.000     484.549     607.819\n",
      "E_2         3144.0352    361.968      8.686      0.000    2413.025    3875.045\n",
      "E_3         2996.2103    411.753      7.277      0.000    2164.659    3827.762\n",
      "M           6883.5310    313.919     21.928      0.000    6249.559    7517.503\n",
      "==============================================================================\n",
      "Omnibus:                        2.293   Durbin-Watson:                   2.237\n",
      "Prob(Omnibus):                  0.318   Jarque-Bera (JB):                1.362\n",
      "Skew:                          -0.077   Prob(JB):                        0.506\n",
      "Kurtosis:                       2.171   Cond. No.                         33.5\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "data_lm=sm.formula.ols('S~X+E_2+E_3+M',data=data_dummies).fit()\n",
    "print(data_lm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 带有自变量函数的回归分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所谓自变量函数，其实就是将原自变量进行一种变换。变换后的变量一方面可能使模型对数据的拟合效果更佳；另一方面也可以满足一些我们实际的数据分析需求。在这一章节，我们简单介绍常见的自变量函数：对数化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 对数化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 对数化变量系数的解释**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 对数变换的作用——一些经验主义**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对数变换是线性回归中非常常见的变量变换，它的作用非常明显：\n",
    "\n",
    "1. 正如上面的例子所示，对数变换可以方便地计算变换百分比，于“价格”型变量而言，百分比解释比绝对值解释更有经济意义。\n",
    "2. 当**因变量**为严格取正的变量，它的分布一般存在异方差性或偏态性，这容易违背CLM假设的同方差/正态性假设。而对数变换可以缓和这种情况。\n",
    "\n",
    "然而，对数变换并不能滥用，因为在一些情况下对数变换会产生极端值。首先，存在负值的变量不可以对数变换；其次，当原变量$y$有部分取值位于[0,1]区间时，$log(y)$的负数值会非常大！而线性模型对极端值是非常敏感的，这会影响模型的效果。\n",
    "\n",
    "对于变量何时取对数，没有一个准确的标准，但在长久的实践中，我们认为可以遵循以下经验：\n",
    "\n",
    "1. 对于大数值大区间变量（价格类变量、人口变量等），可取对数变换，如：工资、薪水、销售额、企业市值、人口数量、雇员数量等。\n",
    "2. 对于小数值小区间变量（时间类变量等），一般不取对数变换，如：受教育年限、工作年限、年龄等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'lprice'}>]], dtype=object)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAF1CAYAAAAwfzllAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAWCklEQVR4nO3df5BdZ3kf8O8TRILrZWwc060qOywJFMZFxeAdB4a0s4KSGtwG0qFtPAy1B2eUzpAOmWraCtJpyCS0YhqgyYRJ64Qf/oOwpGAKY9OA46BQOi1UAoNsqx47VMRWjQ3BNogSWpm3f9wjaaNK7A/d1bv33s9n5s6e854f97mP9u5X59yzZ6u1FgDg/PqB3gUAwCwSwADQgQAGgA4EMAB0IIABoAMBDAAdCGCYUlX116vq3t51AGdWfg8YAM4/R8AwhapqW+8agO9PAMMEqaojVfWmqrqnqh6tqvdW1VOqaqmqHqyqf15VX03y3hNjK7a9vKpuqaqvVdWfVtVvrlj2+qo6POzzE1X1jC4vEGaIAIbJ89okfyvJjyX5K0n+xTD+l5JckuQZSXav3KCqnpTk1iRfSbKQZEeS5WHZq5K8OcnfTfL0JP85yQc2+TXAzBPAMHl+s7X2QGvtG0nemuS6Yfx7SX6ptfbd1tp3Ttvm6iR/Ock/ba19u7X2Z621zwzL/lGSf91aO9xaO57kXyW50lEwbC4BDJPngRXTX8koWJPka621PzvLNpcn+coQsKd7RpJfr6rHquqxJN9IUhkdJQObxIUaMHkuXzH9I0n+1zD9/X6l4YEkP1JV284Qwg8keWtr7f1jrBFYhSNgmDxvqKrLquqSJL+Y5INr2OZzSR5Ksq+qLhwu3HrJsOzfJXlTVf3VJKmqi6rq721K5cBJAhgmz+8m+WSSLyf54yS/utoGrbUnkvydJM9K8idJHkzyD4ZlH0nytiTLVfXNJHclecWmVA6c5EYcMEGq6kiSn22t/UHvWoBz4wgYADoQwADQgVPQANCBI2AA6EAAA0AH5/VGHJdeemlbWFhY1zbf/va3c+GFF25OQRNEH0b0YUQfRvRhRB9O2Wq9OHjw4Ndba08/07LzGsALCws5cODAurbZv39/lpaWNqegCaIPI/owog8j+jCiD6dstV5U1VfOtswpaADoQAADQAcCGAA6EMAA0IEABoAOBDAAdCCAAaADAQwAHQhgAOhAAANABwIYADoQwADQgQAGgA7O619DYutZ2Htb7xJWdWTftb1LABg7R8AA0IEABoAOBDAAdCCAAaADAQwAHQhgAOhg1QCuqqdU1eeq6otVdXdV/fIw/syq+mxV3V9VH6yqH9z8cgFgOqzlCPi7SV7aWnt+kiuTXFNVL0rytiTvbK09K8mjSW7ctCoBYMqsGsBt5Ngw++Th0ZK8NMmHhvGbk7x6MwoEgGlUrbXVV6p6UpKDSZ6V5F1J/k2S/zYc/aaqLk/yn1przzvDtruT7E6S+fn5q5aXl9dV4LFjxzI3N7eubabRZvXh0NHHx77Pcdu546KT074fRvRhRB9G9OGUrdaLXbt2HWytLZ5p2ZpuRdlaeyLJlVV1cZKPJHnuWp+8tXZTkpuSZHFxsS0tLa110yTJ/v37s95tptFm9eGGSbgV5WuXTk77fhjRhxF9GNGHUyapF+u6Crq19liSTyV5cZKLq+pEgF+W5Oh4SwOA6bWWq6CfPhz5pqouSPLyJIczCuLXDKtdn+Sjm1QjAEydtZyC3p7k5uFz4B9I8nuttVur6p4ky1X1q0m+kOTdm1gnAEyVVQO4tfalJC84w/iXk1y9GUUBwLRzJywA6EAAA0AHAhgAOhDAANCBAAaADgQwAHQggAGgAwEMAB0IYADoQAADQAcCGAA6EMAA0IEABoAOBDAAdCCAAaADAQwAHQhgAOhAAANABwIYADoQwADQgQAGgA4EMAB0IIABoAMBDAAdCGAA6EAAA0AHAhgAOhDAANCBAAaADgQwAHQggAGgAwEMAB0IYADoQAADQAcCGAA6EMAA0IEABoAOBDAAdCCAAaADAQwAHQhgAOhg1QCuqsur6lNVdU9V3V1VbxzG31JVR6vqzuHxys0vFwCmw7Y1rHM8yZ7W2uer6qlJDlbV7cOyd7bWfm3zygOA6bRqALfWHkry0DD9rao6nGTHZhcGANOsWmtrX7lqIcmnkzwvyT9JckOSbyY5kNFR8qNn2GZ3kt1JMj8/f9Xy8vK6Cjx27Fjm5ubWtc002qw+HDr6+Nj3OW47d1x0ctr3w4g+jOjDiD6cstV6sWvXroOttcUzLVtzAFfVXJI/SvLW1totVTWf5OtJWpJfSbK9tfb677ePxcXFduDAgXUVv3///iwtLa1rm2m0WX1Y2Hvb2Pc5bkf2XXty2vfDiD6M6MOIPpyy1XpRVWcN4DVdBV1VT07y4STvb63dkiSttYdba0+01r6X5LeTXD2uggFg2q3lKuhK8u4kh1tr71gxvn3Faj+d5K7xlwcA02ktV0G/JMnrkhyqqjuHsTcnua6qrszoFPSRJD+3CfUBwFRay1XQn0lSZ1j08fGXAwCzwZ2wAKADAQwAHQhgAOhAAANABwIYADoQwADQgQAGgA4EMAB0IIABoAMBDAAdCGAA6EAAA0AHAhgAOhDAANCBAAaADgQwAHQggAGgAwEMAB0IYADoQAADQAcCGAA6EMAA0IEABoAOBDAAdLCtdwHTbmHvbWPZz56dx3PDmPYFQH+OgAGgAwEMAB0IYADoQAADQAcCGAA6EMAA0IEABoAOBDAAdCCAAaADAQwAHQhgAOhAAANABwIYADoQwADQgQAGgA4EMAB0sGoAV9XlVfWpqrqnqu6uqjcO45dU1e1Vdd/w9WmbXy4ATIe1HAEfT7KntXZFkhcleUNVXZFkb5I7WmvPTnLHMA8ArMGqAdxae6i19vlh+ltJDifZkeRVSW4eVrs5yas3qUYAmDrVWlv7ylULST6d5HlJ/qS1dvEwXkkePTF/2ja7k+xOkvn5+auWl5fXVeCxY8cyNze3rm22kkNHHx/LfuYvSB7+zlh2NXF27rjo5PSkfz+Miz6M6MOIPpyy1Xqxa9eug621xTMtW3MAV9Vckj9K8tbW2i1V9djKwK2qR1tr3/dz4MXFxXbgwIG1V55k//79WVpaWtc2W8nC3tvGsp89O4/n7Ye2jWVfk+bIvmtPTk/698O46MOIPozowylbrRdVddYAXtNV0FX15CQfTvL+1totw/DDVbV9WL49ySPjKBYAZsFaroKuJO9Ocri19o4Viz6W5Pph+vokHx1/eQAwndZyTvMlSV6X5FBV3TmMvTnJviS/V1U3JvlKkr+/KRUCwBRaNYBba59JUmdZ/LLxlgMAs8GdsACgAwEMAB0IYADoQAADQAcCGAA6EMAA0IEABoAOBDAAdCCAAaADAQwAHQhgAOhAAANABwIYADoQwADQgQAGgA4EMAB0IIABoAMBDAAdCGAA6EAAA0AHAhgAOhDAANCBAAaADgQwAHSwrXcBsJqFvbednN6z83huWDG/VRzZd23vEoAJ4wgYADoQwADQgQAGgA4EMAB0IIABoAMBDAAdCGAA6EAAA0AHAhgAOhDAANCBAAaADgQwAHQggAGgAwEMAB0IYADoQAADQAerBnBVvaeqHqmqu1aMvaWqjlbVncPjlZtbJgBMl7UcAb8vyTVnGH9na+3K4fHx8ZYFANNt1QBurX06yTfOQy0AMDOqtbb6SlULSW5trT1vmH9LkhuSfDPJgSR7WmuPnmXb3Ul2J8n8/PxVy8vL6yrw2LFjmZubW9c2W8mho4+PZT/zFyQPf2csu5poW7UPO3dcdF6fb9LfF+OiDyP6cMpW68WuXbsOttYWz7RsowE8n+TrSVqSX0myvbX2+tX2s7i42A4cOLCO0pP9+/dnaWlpXdtsJQt7bxvLfvbsPJ63H9o2ln1Nsq3ahyP7rj2vzzfp74tx0YcRfThlq/Wiqs4awBu6Crq19nBr7YnW2veS/HaSq8+lQACYNRsK4KravmL2p5PcdbZ1AYD/36rn8qrqA0mWklxaVQ8m+aUkS1V1ZUanoI8k+bnNKxEAps+qAdxau+4Mw+/ehFoAYGa4ExYAdCCAAaADAQwAHQhgAOhAAANABwIYADoQwADQgQAGgA4EMAB0IIABoAMBDAAdCGAA6EAAA0AHAhgAOhDAANCBAAaADgQwAHQggAGgAwEMAB0IYADoQAADQAcCGAA6EMAA0IEABoAOBDAAdCCAAaADAQwAHQhgAOhAAANABwIYADoQwADQgQAGgA4EMAB0IIABoAMBDAAdCGAA6EAAA0AHAhgAOhDAANCBAAaADgQwAHSwagBX1Xuq6pGqumvF2CVVdXtV3Td8fdrmlgkA02UtR8DvS3LNaWN7k9zRWnt2kjuGeQBgjVYN4Nbap5N847ThVyW5eZi+Ocmrx1sWAEy3jX4GPN9ae2iY/mqS+THVAwAzoVprq69UtZDk1tba84b5x1prF69Y/mhr7YyfA1fV7iS7k2R+fv6q5eXldRV47NixzM3NrWubreTQ0cfHsp/5C5KHvzOWXU20rdqHnTsuOq/PN+nvi3HRhxF9OGWr9WLXrl0HW2uLZ1q2bYP7fLiqtrfWHqqq7UkeOduKrbWbktyUJIuLi21paWldT7R///6sd5ut5Ia9t41lP3t2Hs/bD230n2t6bNU+HHnt0nl9vkl/X4yLPozowymT1IuNnoL+WJLrh+nrk3x0POUAwGxYy68hfSDJf03ynKp6sKpuTLIvycur6r4kf3OYBwDWaNVzea21686y6GVjrgUAZoY7YQFABwIYADoQwADQgQAGgA4EMAB0IIABoAMBDAAdCGAA6EAAA0AHAhgAOhDAANCBAAaADgQwAHQggAGgAwEMAB2s+veAgdUt7L3tvD7fnp3Hc8M6nvPIvms3sRpgIxwBA0AHAhgAOhDAANCBAAaADgQwAHQggAGgAwEMAB0IYADoQAADQAcCGAA6EMAA0IEABoAOBDAAdCCAAaADAQwAHQhgAOhAAANABwIYADoQwADQgQAGgA4EMAB0IIABoAMBDAAdCGAA6EAAA0AH285l46o6kuRbSZ5Icry1tjiOogBg2p1TAA92tda+Pob9AMDMcAoaADo41wBuST5ZVQeravc4CgKAWVCttY1vXLWjtXa0qv5iktuT/OPW2qdPW2d3kt1JMj8/f9Xy8vK6nuPYsWOZm5vbcI29HTr6+Fj2M39B8vB3xrKriaYPI+vtw84dF21eMR1N+s+HcdGHU7ZaL3bt2nXwbNdHnVMA/7kdVb0lybHW2q+dbZ3FxcV24MCBde13//79WVpaOrfiOlrYe9tY9rNn5/G8/dA4PrKfbPowst4+HNl37SZW08+k/3wYF304Zav1oqrOGsAbPgVdVRdW1VNPTCf5ySR3bXR/ADBLzuVQYj7JR6rqxH5+t7X2+2OpCgCm3IYDuLX25STPH2MtADAz/BoSAHQggAGgAwEMAB0IYADoQAADQAfuaAAzYFw3hNlM03qzEDgbR8AA0IEABoAOBDAAdCCAAaADAQwAHQhgAOhAAANABwIYADoQwADQgQAGgA4EMAB0IIABoAMBDAAdCGAA6EAAA0AHAhgAOhDAANCBAAaADgQwAHSwrXcB52Jh7229SwBgjc7Hz+w9O4/nhnN4niP7rh1jNd+fI2AA6EAAA0AHAhgAOhDAANCBAAaADgQwAHQggAGgAwEMAB0IYADoYKLvhAVMj43cJelc73o0LU704XzexYlz5wgYADoQwADQgQAGgA4EMAB0IIABoINzCuCquqaq7q2q+6tq77iKAoBpt+EArqonJXlXklckuSLJdVV1xbgKA4Bpdi5HwFcnub+19uXW2v9JspzkVeMpCwCm27kE8I4kD6yYf3AYAwBWUa21jW1Y9Zok17TWfnaYf12SH2+t/fxp6+1OsnuYfU6Se9f5VJcm+fqGipwu+jCiDyP6MKIPI/pwylbrxTNaa08/04JzuRXl0SSXr5i/bBj7c1prNyW5aaNPUlUHWmuLG91+WujDiD6M6MOIPozowymT1ItzOQX935M8u6qeWVU/mORnknxsPGUBwHTb8BFwa+14Vf18kk8keVKS97TW7h5bZQAwxc7pryG11j6e5ONjquVsNnz6esrow4g+jOjDiD6M6MMpE9OLDV+EBQBsnFtRAkAH3QO4qt5TVY9U1V0rxi6pqtur6r7h69OG8aqq3xhuffmlqnphv8rHp6our6pPVdU9VXV3Vb1xGJ+pPiRJVT2lqj5XVV8cevHLw/gzq+qzw2v+4HDhX6rqh4b5+4flC11fwBhV1ZOq6gtVdeswP3M9SJKqOlJVh6rqzqo6MIzN4nvj4qr6UFX9j6o6XFUvnrU+VNVzhu+DE49vVtUvTGofugdwkvcluea0sb1J7mitPTvJHcN8Mrrt5bOHx+4kv3Weatxsx5Psaa1dkeRFSd5Qo9t6zlofkuS7SV7aWnt+kiuTXFNVL0rytiTvbK09K8mjSW4c1r8xyaPD+DuH9abFG5McXjE/iz04YVdr7coVv14yi++NX0/y+6215yZ5fkbfGzPVh9bavcP3wZVJrkryv5N8JJPah9Za90eShSR3rZi/N8n2YXp7knuH6X+f5LozrTdNjyQfTfJyfchfSPL5JD+e0S/WbxvGX5zkE8P0J5K8eJjeNqxXvWsfw2u/LKMfJC9NcmuSmrUerOjFkSSXnjY2U++NJBcl+Z+n/7vOWh9Oe+0/meS/THIftsIR8JnMt9YeGqa/mmR+mJ76218Opw9fkOSzmdE+DKde70zySJLbk/xxksdaa8eHVVa+3pO9GJY/nuSHz2vBm+PfJvlnSb43zP9wZq8HJ7Qkn6yqgzW6s14ye++NZyb5WpL3Dh9L/E5VXZjZ68NKP5PkA8P0RPZhqwbwSW3035aZuFS7quaSfDjJL7TWvrly2Sz1obX2RBudYrosoz/68dy+FZ1fVfW3kzzSWjvYu5Yt4idaay/M6HTiG6rqb6xcOCPvjW1JXpjkt1prL0jy7Zw6zZpkZvqQJBmuf/ipJP/h9GWT1IetGsAPV9X2JBm+PjKMr+n2l5Ooqp6cUfi+v7V2yzA8c31YqbX2WJJPZXS69eKqOvF76ytf78leDMsvSvKn57fSsXtJkp+qqiMZ/ZWxl2b0+d8s9eCk1trR4esjGX3ed3Vm773xYJIHW2ufHeY/lFEgz1ofTnhFks+31h4e5ieyD1s1gD+W5Pph+vqMPhM9Mf4PhyvbXpTk8RWnHSZWVVWSdyc53Fp7x4pFM9WHJKmqp1fVxcP0BRl9Fn44oyB+zbDa6b040aPXJPnD4X/AE6u19qbW2mWttYWMTrP9YWvttZmhHpxQVRdW1VNPTGf0ud9dmbH3Rmvtq0keqKrnDEMvS3JPZqwPK1yXU6efk0ntQ+8PoTNq4kNJ/m9G/8u7MaPPr+5Icl+SP0hyybBuJXlXRp8JHkqy2Lv+MfXgJzI6ZfKlJHcOj1fOWh+G1/bXknxh6MVdSf7lMP6jST6X5P6MTjv90DD+lGH+/mH5j/Z+DWPux1KSW2e1B8Nr/uLwuDvJLw7js/jeuDLJgeG98R+TPG1G+3BhRmd4LloxNpF9cCcsAOhgq56CBoCpJoABoAMBDAAdCGAA6EAAA0AHAhgAOhDAANCBAAaADv4fFcV1lVKafNQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAF1CAYAAAAwfzllAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAU2ElEQVR4nO3dfYylZ3kf4N8dDInrpbaJk6lrXJamlMRhw4enhJQqnS0JpTipIUpTu4lrAmjTKqBUWG1cVKUoKJIrFYhSNW0okNA2sE35CMgmAQuyoDQBdZcAa6CEj26EN8Yu2BjWsZou3P1jzkrDMrtzdubMeebMXJc0mnPej/Pe98x55rfve84+p7o7AMB8fcvoAgBgLxLAADCAAAaAAQQwAAwggAFgAAEMAAMIYFgQVXWiqn5ok/t+vKpWZlsRsBUXjS4A2H7d/b2jawC+kTNg2MWqyj+yYYcSwLBgquoVVfWWqvpvVfXVqvpwVT15zfoTVfXzVfWxJA9V1UVrL19X1SOq6uVV9dnJ/seq6urJuu+uqjur6v6q+lRV/cSgNmHXE8CwmK5P8t+TPCbJm5L8dlU9cs36G5Ncl+Sy7j591r4vm6x/bpK/mOSFSf6sqi5Jcufk8b4zyQ1JfrWqrtnORmCvEsCwmI5191u6+/8leXWSb0vyjDXrf6W7P9/dD6+z74uT/Kvu/lSv+mh3fynJjyQ50d2/3t2nu/uPkrw1yT/Y7mZgL/L6ECymz5+50d1fr6q7k/zl9dav4+okn11n+eOSfH9VfXnNsouS/Jct1AmcgwCGxXT1mRtV9S1JHpvkT9esP9/HnH0+yXcluWud5e/v7h+eVZHAubkEDYvp2qr6scm7nP9Zkv+b5INT7vu6JK+sqifUqu+rqm9PcnuSv15VN1XVIydff6Oqvmd7WoC9TQDDYnpHkn+Y5IEkNyX5scnrwdN4dZLfSvKeJF9J8vokF3f3V5M8O6tvvvrTJF9I8m+SfOtsSweSpLrPd6UK2Gmq6hVJ/lp3/9ToWoDNcwYMAAMIYAAYwCVoABjAGTAADCCAAWCAuU7EccUVV/T+/fvnecg89NBDueSSS+Z6zHnQ12LR12LR12LZyX0dO3bsi939Heutm2sA79+/P0ePHp3nIXPkyJGsrKzM9ZjzoK/Foq/Foq/FspP7qqo/Odc6l6ABYAABDAADCGAAGEAAA8AAAhgABhDAADCAAAaAAQQwAAwggAFgAAEMAAMIYAAYQAADwAACGAAGmOunIcFutf/WO867/pYDp/OCDbbZTiduu27YsYH1OQMGgAEEMAAMIIABYAABDAADCGAAGEAAA8AAAhgABhDAADDAhgFcVVdX1e9V1Seq6uNV9XOT5a+oqpNV9ZHJ13O3v1wA2B2mmQnrdJJbuvvDVfXoJMeq6s7Jutd097/dvvIAYHfaMIC7+54k90xuf7WqPpnkqu0uDAB2s+ru6Teu2p/kA0melORlSV6Q5CtJjmb1LPmBdfY5lORQkiwtLV17+PDhLRd9IU6dOpV9+/bN9ZjzoK+d5fjJB8+7funi5N6H51TMOg5cdem2PO6i/r42oq/FspP7Onjw4LHuXl5v3dQBXFX7krw/yS9199uqainJF5N0klcmubK7X3i+x1heXu6jR49eUPFbdeTIkaysrMz1mPOgr51lmg9jeNXxcZ99sl0fxrCov6+N6Gux7OS+quqcATzVu6Cr6pFJ3prkN7v7bUnS3fd299e6++tJ/lOSp8+qYADY7aZ5F3QleX2ST3b3q9csv3LNZs9PctfsywOA3Wmaa2LPTHJTkuNV9ZHJspcnubGqnpLVS9AnkvzMNtQHALvSNO+C/v0ktc6qd82+HADYG8yEBQADCGAAGEAAA8AAAhgABhDAADCAAAaAAQQwAAwggAFgAAEMAAMIYAAYQAADwAACGAAGEMAAMIAABoABBDAADCCAAWAAAQwAAwhgABhAAAPAAAIYAAYQwAAwgAAGgAEEMAAMIIABYAABDAADCGAAGEAAA8AAAhgABhDAADCAAAaAAQQwAAwggAFgAAEMAAMIYAAYQAADwAACGAAGEMAAMIAABoABBDAADCCAAWAAAQwAAwhgABhAAAPAAAIYAAYQwAAwgAAGgAEEMAAMIIABYAABDAADCGAAGGDDAK6qq6vq96rqE1X18ar6ucnyx1TVnVX16cn3y7e/XADYHaY5Az6d5JbuvibJM5L8bFVdk+TWJO/t7ickee/kPgAwhQ0DuLvv6e4PT25/Ncknk1yV5Pokb5xs9sYkz9umGgFg17mg14Cran+Spyb5UJKl7r5nsuoLSZZmWxoA7F7V3dNtWLUvyfuT/FJ3v62qvtzdl61Z/0B3f9PrwFV1KMmhJFlaWrr28OHDMyl8WqdOncq+ffvmesx52Et9HT/54KBqZmfp4uTeh8cd/8BVl27L4+6l5+FuoK/5O3jw4LHuXl5v3VQBXFWPTHJ7knd396snyz6VZKW776mqK5Mc6e4nnu9xlpeX++jRoxfcwFYcOXIkKysrcz3mPOylvvbfeseYYmbolgOn86rjFw07/onbrtuWx91Lz8PdQF/zV1XnDOBp3gVdSV6f5JNnwnfinUlunty+Ock7tlooAOwV0/yT/JlJbkpyvKo+Mln28iS3JfmtqnpRkj9J8hPbUiEA7EIbBnB3/36SOsfqZ822HADYG8yEBQADCGAAGEAAA8AAAhgABhDAADCAAAaAAQQwAAwwbm48YG62azrPWw6czgtm9NjbNV0m7FTOgAFgAAEMAAMIYAAYQAADwAACGAAGEMAAMIAABoABBDAADCCAAWAAAQwAAwhgABhAAAPAAAIYAAYQwAAwgAAGgAEEMAAMIIABYAABDAADCGAAGEAAA8AAAhgABhDAADCAAAaAAQQwAAwggAFgAAEMAAMIYAAYQAADwAACGAAGEMAAMIAABoABBDAADCCAAWAAAQwAAwhgABhAAAPAAAIYAAYQwAAwgAAGgAEEMAAMIIABYAABDAADCGAAGGDDAK6qN1TVfVV115plr6iqk1X1kcnXc7e3TADYXaY5A/6NJM9ZZ/lruvspk693zbYsANjdNgzg7v5AkvvnUAsA7BlbeQ34JVX1sckl6stnVhEA7AHV3RtvVLU/ye3d/aTJ/aUkX0zSSV6Z5MrufuE59j2U5FCSLC0tXXv48OHZVD6lU6dOZd++fXM95jzspb6On3xwUDWzs3Rxcu/Do6uYvVn2deCqS2fzQDOwl8bXbrCT+zp48OCx7l5eb92mAnjadWdbXl7uo0ePbni8WTpy5EhWVlbmesx52Et97b/1jjHFzNAtB07nVccvGl3GzM2yrxO3XTeTx5mFvTS+doOd3FdVnTOAN3UJuqquXHP3+UnuOte2AMA32/CfrlX15iQrSa6oqruT/OskK1X1lKxegj6R5Ge2r0QA2H02DODuvnGdxa/fhloAYM8wExYADCCAAWAAAQwAAwhgABhAAAPAAAIYAAYQwAAwgAAGgAEEMAAMIIABYAABDAADCGAAGEAAA8AAAhgABhDAADCAAAaAAQQwAAwggAFgAAEMAAMIYAAYQAADwAACGAAGEMAAMIAABoABBDAADCCAAWAAAQwAAwhgABhAAAPAAAIYAAYQwAAwgAAGgAEEMAAMIIABYAABDAADCGAAGEAAA8AAAhgABhDAADCAAAaAAQQwAAwggAFgAAEMAAMIYAAYQAADwAACGAAGEMAAMIAABoABBDAADCCAAWAAAQwAA2wYwFX1hqq6r6ruWrPsMVV1Z1V9evL98u0tEwB2l2nOgH8jyXPOWnZrkvd29xOSvHdyHwCY0oYB3N0fSHL/WYuvT/LGye03JnnebMsCgN2tunvjjar2J7m9u580uf/l7r5scruSPHDm/jr7HkpyKEmWlpauPXz48EwKn9apU6eyb9++uR5zHvZSX8dPPjiomtlZuji59+HRVczeLPs6cNWls3mgGdhL42s32Ml9HTx48Fh3L6+37qKtPnh3d1WdM8W7+7VJXpsky8vLvbKystVDXpAjR45k3sech73U1wtuvWNMMTN0y4HTedXxLQ+3HWeWfZ34yZWZPM4s7KXxtRssal+bfRf0vVV1ZZJMvt83u5IAYPfbbAC/M8nNk9s3J3nHbMoBgL1hmv+G9OYkf5jkiVV1d1W9KMltSX64qj6d5Icm9wGAKW344k1333iOVc+acS0AsGeYCQsABhDAADCAAAaAAQQwAAwggAFgAAEMAAPsvrnxgIW0fwdNOXrLgdPfNAXqiduuG1QNu5UzYAAYQAADwAACGAAGEMAAMIAABoABBDAADCCAAWAAAQwAAwhgABhAAAPAAAIYAAYQwAAwgAAGgAEEMAAMIIABYAABDAADCGAAGEAAA8AAAhgABhDAADCAAAaAAQQwAAwggAFgAAEMAAMIYAAYQAADwAACGAAGEMAAMIAABoABBDAADCCAAWAAAQwAAwhgABhAAAPAAAIYAAa4aHQBAItg/613jC5hQyduu250CVwAZ8AAMIAABoABBDAADCCAAWAAAQwAAwhgABhAAAPAAAIYAAbY0kQcVXUiyVeTfC3J6e5enkVRALDbzWImrIPd/cUZPA4A7BkuQQPAANXdm9+56n8neSBJJ/m17n7tOtscSnIoSZaWlq49fPjwpo+3GadOncq+ffvmesx52Et9HT/54KBqZmfp4uTeh0dXMXv62lkOXHXpedfvpb8bO8XBgwePnevl2a0G8FXdfbKqvjPJnUle2t0fONf2y8vLffTo0U0fbzOOHDmSlZWVuR5zHvZSX4swCf5GbjlwOq86vvs++0RfO8tGH8awl/5u7BRVdc4A3tIl6O4+Ofl+X5K3J3n6Vh4PAPaKTQdwVV1SVY8+czvJs5PcNavCAGA328o1lqUkb6+qM4/zpu7+3ZlUBQC73KYDuLs/l+TJM6wFAPYM/w0JAAYQwAAwgAAGgAEEMAAMIIABYAABDAADCGAAGEAAA8AAAhgABhDAADCAAAaAAQQwAAwggAFgAAEMAAMIYAAYQAADwAACGAAGEMAAMMBFowtgrP233jG6hG9wy4HTecEOqwlgOzgDBoABBDAADCCAAWAAAQwAAwhgABhAAAPAAAIYAAYQwAAwgAAGgAEEMAAMIIABYABzQQMwF9s19/ws55A/cdt1M3mcaTgDBoABBDAADCCAAWAAAQwAAwhgABhAAAPAAAIYAAYQwAAwgAAGgAEEMAAMIIABYICFngt6mnlFZzlH6E6yW/sCNm+jv4n+buwszoABYAABDAADCGAAGEAAA8AAAhgABhDAADCAAAaAAbYUwFX1nKr6VFV9pqpunVVRALDbbTqAq+oRSf59kr+X5JokN1bVNbMqDAB2s62cAT89yWe6+3Pd/edJDie5fjZlAcDutpUAvirJ59fcv3uyDADYQHX35nas+vEkz+nuF0/u35Tk+7v7JWdtdyjJocndJyb51ObL3ZQrknxxzsecB30tFn0tFn0tlp3c1+O6+zvWW7GVD2M4meTqNfcfO1n2Dbr7tUleu4XjbElVHe3u5VHH3y76Wiz6Wiz6WiyL2tdWLkH/zyRPqKrHV9WjktyQ5J2zKQsAdrdNnwF39+mqekmSdyd5RJI3dPfHZ1YZAOxiW/o84O5+V5J3zaiW7TLs8vc209di0ddi0ddiWci+Nv0mLABg80xFCQADLHQAV9UjquqPqur2dda9pqo+Mvn646r68pp1X1uzbse9cayqTlTV8Ul9R9dZX1X1K5MpQD9WVU9bs+7mqvr05Ovm+VZ+flP09ZOTfo5X1R9U1ZOn3XekKfpaqaoH1zznfmHNuh07nesUff3zNT3dNRlXj5lm35Gq6rKqektV/a+q+mRV/cBZ6xd1fG3U16KOr436WsjxlSTp7oX9SvKyJG9KcvsG2700q28SO3P/1OjaN6j3RJIrzrP+uUl+J0kleUaSD02WPybJ5ybfL5/cvnx0PxfQ1988U29Wpzj90LT77vC+VtZ7jmb1zYufTfJXkzwqyUeTXDO6n838zJP8aJL3Lcjv641JXjy5/agkl521flHH10Z9Ler42qivhRxf3b24Z8BV9dgk1yV53RSb35jkzdtb0Vxdn+Q/96oPJrmsqq5M8neT3Nnd93f3A0nuTPKckYVeiO7+g0ndSfLBrP7f8t1sN03nuhBjrKouTfKDSV6fJN3959395bM2W7jxNU1fizi+pvx9ncuOH18LG8BJfjnJv0jy9fNtVFWPS/L4JO9bs/jbqupoVX2wqp63bRVuXid5T1Udq9WZxM52rmlAd/r0oBv1tdaLsnoWspl9522a2n6gqj5aVb9TVd87WbYrfl9V9ReyGkRvvdB9B3h8kv+T5Ndr9eWr11XVJWdts4jja5q+1lqU8TVtX4s4vhYzgKvqR5Lc193Hptj8hiRv6e6vrVn2uF6dNeUfJfnlqvqu7ahzC/5Wdz8tq5eJfraqfnB0QTMyVV9VdTCrfyB+/kL3HWSj2j6c1efck5P8uyS/Pef6Nmvan/mPJvkf3X3/Jvadt4uSPC3Jf+jupyZ5KMnOe23wwk3d14KNr2n6WtTxtZgBnOSZSf5+VZ3I6mWFv1NV//Uc296Qsy6NdffJyffPJTmS5KnbVukmrKnvviRvz+qllLXONQ3oVNODjjJFX6mq78vqywrXd/eXLmTfUTaqrbu/0t2nJrffleSRVXVFdsHva+J8Y2yn/b7uTnJ3d39ocv8tWf0Dv9Yijq9p+lrE8bVhX4s6vpIFDeDu/pfd/dju3p/Vwf++7v6ps7erqu/O6psl/nDNssur6lsnt6/Iaph/Yi6FT6GqLqmqR5+5neTZSe46a7N3JvnHk3drPiPJg919T1ZnJXv2pMfLJ/u+e47ln9M0fVXVX0nytiQ3dfcfX8i+o0zZ11+qqprcfnpWx92XsoOnc532Zz55je5vJ3nHhe47Qnd/Icnnq+qJk0XPyjeP/4UbX9P0tYjja8q+Fm58nbGlmbB2mqr6xSRHu/vMD/mGJId78pa4ie9J8mtV9fWs/qJu6+4dE8BJlpK8ffJ8uijJm7r7d6vqnyRJd//HrM4+9twkn0nyZ0l+erLu/qp6ZVafeEnyi2ddFhxpmr5+Icm3J/nVyXanJy8VrLvv/FtY1zR9/XiSf1pVp5M8nOSGyXNyJ0/nOk1fSfL8JO/p7oc22ndulW/spUl+c/JH+XNJfnoXjK9k474WcXwlG/e1iOMriZmwAGCIhbwEDQCLTgADwAACGAAGEMAAMIAABoABBDAADCCAAWAAAQwAA/x/CSlx8tnk9IwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 对数化对正态化的作用\n",
    "## 以hprice1的price为例\n",
    "### 未经对数化的直方图\n",
    "hprice1.hist(column='price',figsize=(8,6))\n",
    "\n",
    "### 对数化后的直方图\n",
    "hprice1.hist(column='lprice',figsize=(8,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显然，对数化后的数据分布更接近正态分布！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.假设误差分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 OLS估计新求解法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新解法的效果\n",
      "0.009426012260472114\n"
     ]
    }
   ],
   "source": [
    "# 以gpa_lm2的ACT系数为例\n",
    "\n",
    "# 新求解法\n",
    "gpa_lm2_pre=sm.formula.ols('ACT~hsGPA',data=gpa1).fit()\n",
    "gpa1['resid']=gpa_lm2_pre.resid\n",
    "gpa_lm2_pre2=sm.formula.ols('colGPA~resid-1',data=gpa1).fit()\n",
    "print('新解法的效果')\n",
    "print(gpa_lm2_pre2.params[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 估计系数的方差构成与多重共线性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在MLR.1~MLR.5下，$\\operatorname{Var}\\left(\\hat{\\beta}_{1}\\right)$可计算为\n",
    "$$\n",
    "\\operatorname{Var}\\left(\\hat{\\beta}_{1}\\right)=\\frac{\\sigma^{2}}{\\sum_{i=1}^{n} \\hat{r}_{i 1}^{2}}=\\frac{\\sigma^{2}}{S S T_{1}\\left(1-R_{1}^{2}\\right)}\n",
    "$$\n",
    "其中，$S S T_{1}=\\sum_{i=1}^{n}\\left(x_{i 1}-\\bar{x}_{1}\\right)^{2}$为$x_1$变量本身的完全平方和，$R_{1}^{2}$是$x_1$对其他解释变量所作回归的拟合优度判决系数，$\\sigma^{2}$则是之前提及的随机误差的方差。至此，回归系数估计方差的构成就结构完毕了，它依赖于三个因素：\n",
    "\n",
    "**· 误差方差$\\sigma^{2}$**，随机误差的方差可以理解为数据集的噪声信息，噪声信息越多，估计的不确定性就越大，方差就越大，这点很顺理成章！但是一般来说，对于给定的数据集，误差方差也是随之确定的，我们一般不会在这个因素上下文章。\n",
    "\n",
    "**· 自变量$x_1$本身的总变异$SST_1$**，自变量总变异越大，表明自变量散步程度越高，估计越牢靠。关于这个指标，我们会发现：对于一个分析任务来说，样本量越大，总变异也就越大，进而估计方差会变小，这告诉我们更多的随机样本有利于提高估计的精度！\n",
    "\n",
    "**· 自变量间的线性关联程度$R_{1}^{2}$**，$R_{1}^{2}$约接近1，$x_1$与其他自变量之间的线性关系就越强烈，估计方差也越大。这种近似的共线性关系被称为**多重共线性(multicolinearity)**，它不同于完全共线性（事实上$R_{1}^{2}=1$时就是完全共线性），这种现象在数据分析中普遍存在，只是程度有所区别。那么如何衡量共线性的严重程度呢？我们一般用方差膨胀因子(VIF,Variance Inflation Factor)来评判\n",
    "$$\n",
    "VIF_{x1}=\\frac{1}{1-R_{1}^{2}}\n",
    "$$\n",
    "若$VIR>10$，意味着共线性很严重，需要采取措施降低共线性。\n",
    "\n",
    "现在的问题是，如何降低多重共线性呢？一种方法是使变量之间尽可能的不相关，关于这点我们可以查看变量之间的皮尔逊相关系数，如果某两个变量之间的相关系数非常高，我们就需要考虑不把它们同时放入模型中；第二种方法是减少自变量的个数。一般而言，模型中自变量个数越多，R方会越大。而$R_{1}^{2}$正是$x_1$对其他自变量做回归的判决系数，显然，其他自变量越多，$R_{1}^{2}$就越大。这个方法告诉我们，**模型的自变量绝非越多越好**，有关这个问题的讨论，我们会在下一节做深入探讨。\n",
    "\n",
    "最后，我们总结一下降低估计方差，提高估计精度的方法\n",
    "\n",
    "1. 采取更合理的数据采样方法，降低数据噪声。\n",
    "2. 增大数据样本量\n",
    "3. 根据线性相关程度筛选纳入模型的变量，且切忌纳入过多变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 模型误设的误差分析——违反MLR.1的后果是什么"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 如何理解模型误设"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在MLR.1中，我们假设自己设置的模型是“正确的”，即对\n",
    "$$\n",
    "y=\\beta_{0}+\\beta_{1} x_{1}+\\beta_{2} x_{2}+\\cdots+\\beta_{k} x_{k}+u\n",
    "$$\n",
    "的假设上，我们正确地纳入了所有关键的自变量，且没有纳入多余的自变量。而多纳入一个无关的变量，以及少纳入一个关键的变量，都能算是违反MLR.1，我们可以看看误设模型对模型中变量系数的估计会有怎样子的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 变量选择的方法论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们总结一下以上的内容：\n",
    "\n",
    "1. 减少模型中的变量，可能会导致模型其他系数的估计有偏，但是会使系数估计的方差减少，即：增大偏差、减小方差\n",
    "2. 增加模型中的变量，不会导致模型系数估计有偏，但是会使系数估计的方差增大，即：减小偏差、增大方差\n",
    "\n",
    "而在实际情况中，我们不可能得知一个真正正确的模型，而是在摸索中慢慢找到适合纳入模型的变量。通常来说，有两种考量纳入变量的方法论：\n",
    "\n",
    "1. 从少数变量开始，一步步向模型纳入我们认为重要的变量，并通过t检验显著性来判断是否纳入该变量；如果我们认为变量存在二次项效应与交互项效应，可以将其纳入模型并通过联合F检验判断其显著性，直到模型的解释度达到一个较高的水平。\n",
    "2. 从多数变量开始，一步步将不显著的变量剔除出模型。\n",
    "\n",
    "这两种方法该用哪一种，视具体情况而定。假若数据集的变量非常多，那我倾向于使用第一种方法，一步步将我们认为重要的变量纳入模型；若变量不多，则第二种方法可以考虑。\n",
    "\n",
    "对于变量的选择，还有一些自动化的方法，如向前逐步选择、向后逐步选择等等。但我个人认为，这些自动的变量选择方法更关注模型的预测能力，而不注重变量本身对模型的意义，它们更适用于机器学习的预测任务，而不适用于回归分析，因为回归分析中最重要的意义就是“解释与推断”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 模型不满足正态性的分析——违反MLR.6的后果是什么"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 如何理解与观测正态性假设"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看样本因变量$y$的分布图就可以大致判断正态性了。\n",
    "\n",
    "然而，因变量$y$不为正态分布的情况是非常常见的，以下面的$narr86$为例，它统计了青年人在某一特定年份被拘捕的次数。很明显，绝大部分青年人都没有被拘捕过，因此该变量的分布是一个严重的偏态分布，不可能为正态分布。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0     1970\n",
      "1.0      559\n",
      "2.0      121\n",
      "3.0       42\n",
      "5.0       13\n",
      "4.0       12\n",
      "6.0        4\n",
      "12.0       1\n",
      "9.0        1\n",
      "10.0       1\n",
      "7.0        1\n",
      "Name: narr86, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 频数直方图\n",
    "crime1=pd.read_stata('crime1.dta')\n",
    "crime1.hist(column='narr86',figsize=(8,6))\n",
    "print(crime1.narr86.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.异方差下的回归分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 广义最小二乘法\n",
    "\n",
    "回归存在异方差的另一种解决方法，就是根据异方差的具体形式使用广义的最小二乘法，对模型进行重新估计。对于这种解决方法，变量系数的估计也会发生变化，但是在异方差情形下，它比传统的OLS估计法更优！\n",
    "\n",
    "我们提到“根据异方差的具体形式”，是指异方差可以用自变量的函数被表达出来。如果它能被我们找出来，我们就可以是加权最小二乘估计WLS；如果由于函数形式复杂而无法被判断出来，我们则使用可行的广义最小二乘估计FGLS。\n",
    "\n",
    "加权最小二乘法的原理非常简单。\n",
    "\n",
    "假设异方差的形式除去一个常数外是已知的，即\n",
    "$$\n",
    "\\operatorname{Var}(u \\mid x)=\\sigma^{2} h(x)\n",
    "$$\n",
    "那么如果我们在原模型两边同除以$\\frac{1}{\\sqrt{h\\left( x \\right)}}$，即\n",
    "$$\n",
    "\\frac{y}{\\sqrt{h\\left( x \\right)}}=\\frac{\\beta _{0}^{*}}{\\sqrt{h\\left( x \\right)}}+\\beta _{1}^{*}\\frac{x_1}{\\sqrt{h\\left( x \\right)}}+\\cdots +\\beta _{k}^{*}\\frac{x_k}{\\sqrt{h\\left( x \\right)}}+\\frac{u}{\\sqrt{h\\left( x \\right)}}\n",
    "$$\n",
    "\n",
    "并将带有$\\frac{1}{\\sqrt{h\\left( x \\right)}}$的自变量与随机误差视作是新的变量与随机误差，则问题就是同方差情形了，我们可以使用OLS估计。\n",
    "\n",
    "我们使用一个简单的例子教会大家如何鉴别简单的异方差形式，并用statsmodels包中的wls函数进行wls估计。\n",
    "\n",
    "**· Example13.** 我们想研究27家企业主管人数Y与工人人数X的关系。由于只有一个自变量，一开始我们可以考虑简单线性回归模型\n",
    "$$\n",
    "Y=\\beta _0+\\beta _1X+u\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>294</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>247</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>267</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>358</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>423</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     X   Y\n",
       "0  294  30\n",
       "1  247  32\n",
       "2  267  37\n",
       "3  358  44\n",
       "4  423  47"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 载入数据集\n",
    "data=pd.read_table('./data/P176.txt')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'resid_ols | X')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 936x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 在判断异方差函数的形式上，可视化发挥着重要的作用。我们可以采用之前画散点图的办法，初步观测样本分布的情况。\n",
    "# 直接看Y与X的散点图\n",
    "fig=plt.figure(figsize=(13,6))\n",
    "ax1=fig.add_subplot(1,2,1)\n",
    "plt.scatter(data.X,data.Y,axes=ax1)\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('Y')\n",
    "ax1.set_title('Y | X')\n",
    "\n",
    "# 查看ols估计的残差与X的散点图\n",
    "data_lm=sm.formula.ols('Y~X',data=data).fit()\n",
    "ax2=fig.add_subplot(1,2,2)\n",
    "plt.scatter(data.X,data_lm.resid,axes=ax2)\n",
    "ax2.set_xlabel('X')\n",
    "ax2.set_ylabel('resid_ols')\n",
    "ax2.set_title('resid_ols | X')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
