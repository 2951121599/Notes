# 02\_回归

## 0. 概述

介绍线性回归的数学原理以及代码实现

![lr](https://d/NoteBook\_md/images/lr.png)

## 1. 数学推导

### 1.1. 训练目的：求线性回归模型参数

线性模型就是对输入特征加权求和，再加上截距项，以此进行预测。

* 训练目的：求模型参数w
* 训练原理：损失函数最小
* 训练方法：梯度下降等

一元线性回归：

* 一个样本时：y = w0 + w1\*x + b
* 一组样本时：yi = w0 + w1\*xi + bi （i = 1, ... n）

> w0 + w1\*x 为回归函数 b 为随机误差 bi 是第i次观察时随机误差b所取得的值

多元线性回归：

* 一次观察时：y = w0 + w1_x1 + ... + wn_xn + b
* 多次观察时：yi = w0 + w1_x1(i) + ... + wn_xn(i) + b(i)

> 样本和观测可理解为相同意义

公式：线性回归模型预测

$$
\hat{y}=w_0+w_1x_1+...+w_nx_n+b
$$

### 1.2. 训练原理：损失函数

机器学习算法中，一般都会**最大化或者最小化**一个函数，这样的函数被称为“目标函数”。而其中最小化的那一类函数就是损失函数。

度量模型一次预测的好坏，即真实值与预测值的误差

回归模型的性能的评价指标主要有：RMSE(平方根误差)、MAE（平均绝对误差）、MSE(平均平方误差)、R2\_score。但是当量纲不同时，RMSE、MAE、MSE难以衡量模型效果好坏，这就需要用到R2\_score。

#### 1.2.1 均方误差 MSE

![](images/03\_MSE.png)

eg：假设建立了两个模型，一个模型使用了 10 个样本，计算出的上式值是 100；另一个模型使用了 50 个样本，计算出的值是 200，很难判断两个模型哪个更好。

将上式除以 m 就可以去除样本数量的影响：针对上面举例的两个模型，他们的 MSE 分别是 10（100/10）和 4（200/50），所以后者模型效果更好。

#### 1.2.2 均方根误差 RMSE

![](images/04\_RMSE.png)

但是，MSE 公式有一个问题是会改变量纲。因为公式平方了，比如说 y 值的单位是万元，MSE 计算出来的是万元的平方，对于这个值难以解释它的含义。

所以为了消除量纲的影响，我们可以对这个MSE 开方，得到的结果就第二个评价指标：均方根误差 RMSE（Root Mean Squared Error）。可以看到 MSE 和 RMSE 二者是呈正相关的，MSE 值大，RMSE 值也大，所以在评价线性回归模型效果的时候，使用 RMSE 就可以了。

#### 1.2.3 平均绝对值误差 MAE

![](images/05\_MAE.png)

MAE是目标值和预测值之差的绝对值之和。其只衡量了预测值误差的平均模长，而不考虑方向，取值范围也是从0到正无穷

MSE和MAE比较

MSE其实是平方项之和，因此相对于MAE，**如果出现异常值，可能会进一步加大误差**。如果数据集中存在较多异常点，选择MAE作为损失函数可能会更加合适。这其实跟两个误差函数的目标一致的。我们可以考虑极端一点，如果选择一个数来预测误差，对于MSE这个值是数据集的均值；而对于MAE这个值则是中位数。事实上，在生活中我们可以感受到，中位数比平均数更加靠谱（比如说到平均工资，很多人都是被平均的），因此MAE表现可能会更加稳定。但是MAE也会有问题，就是更新梯度其实是一致的，即使当误差很小的情况下，也会使用大梯度进行更新，因此使用MAE时可能需要逐步降低学习率。

两者的应用场景也很明显：**在需要做异常值检测的情况下，考虑使用MSE，因为MSE对异常值很敏感。**

上面三个模型解决了样本数量 m 和 量纲的影响。但是它们都存在一个相同的问题：当量纲不同时，难以衡量模型效果好坏。举个例子，模型在一份房价数据集上预测得到的误差 RMSE 是 5 万元， 在另一份学生成绩数据集上得到误差是 10 分。凭这两个值，很难知道模型到底在哪个数据集上效果好。

总结：

* 做异常值检测的情况，MSE
* 两个模型哪个更好，样本数量不同，RMSE
* 消除量纲的影响，RMSE
* 存在较多异常点，MAE
* 预测平均收入等，MAE

#### 1.2.4 决定系数 R2\_score

![](images/06\_R2\_score.png)

* R2\_score 的取值范围是“负无穷到1”，经常是“0到1”。（很多资料说是0\~1是不准确的）
* R2\_score 不是R的平方，也可能为负数 (分子>分母)
* R2\_score 反映因变量的全部变异能通过回归关系被自变量解释的比例
* R2\_score 比较不同量纲下模型的效果好坏

通俗地理解为**使用均值作为误差基准，看预测误差是否大于或者小于均值基准误差**

通过它的取值可以更好理解它是如何评价模型好坏的，有这几种取值情况：

> \*\*R2\_score = 1，达到最大值。\*\*即分子为 0 ，意味着样本中预测值和真实值完全相等，没有任何误差，表示回归分析中自变量对因变量的解释越好，也就是说我们建立的模型完美拟合了所有真实数据，是效果最好的模型，R2\_score 值也达到了最大。
>
> 但通常模型不会这么完美，总会有误差存在，**当误差很小的时候，分子小于分母，模型会趋近 1**，仍然是好的模型，随着误差越来越大，R2\_score 也会离最大值 1 越来越远，直到出现第 2 中情况。
>
> **R2\_score = 0**。此时分子等于分母，样本的每项预测值都等于均值。也就是说我们辛苦训练出来的模型和前面说的均值模型完全一样，还不如不训练，直接让模型的预测值全去均值。
>
> 当误差越来越大的时候就出现了第三种情况。
>
> **R2\_score < 0** ：**分子大于分母，训练模型产生的误差比使用均值产生的还要大**，也就是训练模型反而不如直接去均值效果好。出现这种情况，通常是模型本身不是线性关系的，而我们误使用了线性模型，导致误差很大。

```python
# 总结 (预测值 真实值 均值 之间的关系)
R2_score < 0：（预测值的误差比均值的还要大）可能不属于线性模型
R2_score = 0：（预测值=均值）
0 < R2_score < 1：（R2_score值越大，预测值越接近真实值）
R2_score = 1：（预测值=真实值，小心过拟合）
```

### 1.3. 训练方法：梯度下降等

回归任务的优化目标就是使得预测值和真实值之间的均方差最小化

#### 1.3.1. 基于最小二乘法

#### 1.3.2. 梯度下降

　

比较线性回归求解的相关算法

| 算法      | 数据集较大 | 数据特征较多 | 超参数 |
| ------- | :---: | :----: | :-: |
| 标准方程    |   快   |    慢   |  0  |
| 批量梯度下降  |   慢   |    快   |  2  |
| 随机梯度下降  |   快   |    快   |  ≥2 |
| 小批量梯度下降 |   快   |    快   |  ≥2 |

## 2. 代码实现

实现方式：

* numpy
* Sklearn
* Tensorflow
* pytorch

在代码实现部分，我们会基于两个框架来实现，一个是`numpy`，一个是`pytorch`，不过都是基于梯度下降来计算的（注意，由于我们的示例数据并不大，因此还是使用批量梯度下降）

### 2.1. 基于numpy的实现

#### 1. 数据准备

这里我们使用sklearn官方提供的diabetes数据集（csv）

```python
def get_data():
    """
    创建需要的数据集，打卡活动中给的数据集较为简单，这里使用sklearn官方提供的diabetes数据集
    :return:返回x(442,10),y(442,)
    """
    data = load_diabetes()
    # 获取输入数据集的特征和标签
    features, targets = data.data, data.target
    X, y = shuffle(features, targets, random_state=42)
    return X, y
```

#### 2. 初始化参数

```python
def initialize_parameters(dims):
    """
    根据输入数据的维度，初始化参数w,b(这里我们直接初始化成0，也可以考虑随机初始化)
    :param dims: 输入数据的维度，如果输入的数据是m*n，则返回的w是n*1，b是1*1
    :return: 返回w,b
    """
    w = np.zeros((dims, 1))
    b = 0
    return w, b
```

#### 3. 建立回归模型

```python
def linear_model(x_train, w, b):
    """
    线性回归模型，返回模型拟合输出y
    :param x_train: 特征矩阵(m,n)
    :param w: 参数矩阵(n, 1)
    :param b: 截距项
    :return: 拟合值y_hat(m, 1)
    """
    return np.dot(x_train, w) + b
```

#### 4. 计算损失函数

```python
def loss_function(y_hat, y):
    """
    根据拟合值和真实值，计算损失
    :param y_hat: 拟合值
    :param y: 真实值
    :return: 均方损失
    """
    # 训练数据集的数量
    num_train = y_hat.shape[0]
    return np.sum((y_hat - y) ** 2) / num_train
```

#### 5. 梯度计算

```python
def get_gradients(x_train, y_train, w, b):
    """
    计算参数的梯度
    :param x_train: 训练数据集特征矩阵
    :param y_train: 训练数据集标签矩阵
    :param w: 参数矩阵
    :param b: 截距项
    :return: 返回参数w,b的梯度，其实就是对MSE求一阶偏导
    """
    num_train = x_train.shape[0]
    y_hat = linear_model(x_train, w, b)
    dw = np.dot(x_train.T, (y_hat - y_train)) / num_train
    db = np.sum((y_hat - y_train)) / num_train
    return dw, db
```

#### 6. 线性回归训练

```python
def linear_train(x_train, y_train, leanring_rate=0, epochs=10000):
    """
    在10000次迭代过程中，不断更新w和b
    :param x_train: 训练数据集特征矩阵
    :param y_train: 训练数据集标签矩阵
    :param leanring_rate: 学习率
    :param epochs: 也就是训练次数
    :return: 记录每200次训练的损失losses，最终训练得到的参数params字典以及最后的梯度字典
    """
    losses = []
    w, b = initialize_parameters(x_train.shape[1])
    for epoch in range(epochs):
        y_hat = linear_model(x_train, w, b)
        loss = loss_function(y_hat, y_train)
        dw, db = get_gradients(x_train, y_train, w, b)
        w += -leanring_rate * dw
        b += -leanring_rate * db
        losses.append(loss)
        if epoch % 200 == 0:
            print(f"第{epoch + 1}次训练，loss={loss:.3f}")
        params = {
            'w': w,
            'b': b
        }
        gradients = {
            'dw': dw,
            'db': db
        }

    return losses, params, gradients
```

#### 7. 绘制训练过程

```python
def plot_train(losses):
    plt.plot(range(len(losses)), losses)
    plt.show()
```

#### 8. 调用相关函数，完成训练

```python
if __name__ == '__main__':
    X, y = get_data()
    # 取70%比例的数据为训练集
    num_train = int(X.shape[0] * 0.7)
    x_train, y_train = X[:num_train], y[:num_train]
    x_test, y_test = X[num_train:], y[num_train:]
    learning_rate = 0.2
    losses, params, gradients = linear_train(x_train, y_train, learning_rate)
    plot_train(losses)
    print(params, gradients)
```

最终得到的损失曲线：

![image-20220518105208159](https://d/NoteBook\_md/images/image-20220518105208159.png)

#### 9. 使用训练得到的参数预测

```python
def predict(x_test, params):
    """
    使用训练的参数对测试集进行测试
    :param x_test: 训练数据集特征矩阵
    :param params: 上述训练过程得到的参数
    :return: 测试集预测值y_pred
    """
    w, b = params.get('w'), params.get('b')
    return linear_model(x_test, w, b)
```

#### 10. 模型评价

除了使用mse衡量模型表现，我们还可以使用$R^2$衡量自变量对因变量的解释情况。

```python
def r_squared_score(y_test, y_pred):
    """
    通过测试集预测值和真实值，计算R^2
    :param y_test: 训练数据集标签矩阵
    :param y_pred: 训练数据集预测矩阵
    :return: R^2 = 1 - (ssr / sse)
    """
    y_mean = np.mean(y_test)
    sse = np.sum((y_test - y_mean) ** 2)
    ssr = np.sum((y_test - y_pred) ** 2)
    r_squared = 1 - (ssr / sse)
    return r_squared
```

最后得到的结果如下：

![image-20220518110939109](https://d/NoteBook\_md/images/image-20220518110939109.png)

### 2.2. 基于pytorch的实现

这部分就直接上代码吧，逻辑跟上一部分Numpy实现其实是一致的，

#### 1. 数据准备

```python
def get_data():
    """
    创建需要的数据集，打卡活动中给的数据集较为简单，这里使用sklearn官方提供的diabetes数据集，
    注意最后结果转换成tensor
    :return:返回x,y
    """
    data = load_diabetes()
    # 获取输入数据集的特征和标签
    features, targets = data.data, data.target
    X, y = shuffle(features, targets, random_state=42)
    return torch.Tensor(X), torch.Tensor(y.reshape((-1, 1)))
```

#### 2. 读取数据集

　　在下面的代码中，我们定义一个`data_iter`函数， 该函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为`batch_size`的小批量。 每个小批量包含一组特征和标签。

```python
def data_iter(batch_size, features, labels):
    """
    返回大小为batch_size的数据集，处理合理大小的“小批量”。 每个样本都可以并行地进行模型计算，
    且每个样本损失函数的梯度也可以被并行计算
    :param batch_size: 批量大小
    :param features: 特征
    :param labels: 标签
    :return: 迭代器，通过遍历获取特征和标签
    """
    num_train = len(features)
    indices = list(range(num_train))
    random.shuffle(indices)
    for i in range(0, num_train, batch_size):
        batch_indices = indices[i: min(i + batch_size, num_train)]
        yield features[batch_indices], labels[batch_indices]
```

#### 3. 初始化参数

　　在初始化参数之后，我们的任务是更新这些参数，直到这些参数足够拟合我们的数据。 每次更新都需要计算损失函数关于模型参数的梯度。 有了这个梯度，我们就可以向减小损失的方向更新每个参数。可以通过`backward()`计算梯度。

```python
def initialize_parameters(dims):
    """
    根据输入数据的维度，初始化参数w,b(这里使用随机初始化)
    :param dims: 输入数据的维度，如果输入的数据是m*n，则返回的w是n*1，b是1*1
    :return: 返回w,b
    """
    w = torch.normal(0, 0.01, size=(dims, 1), requires_grad=True)
    b = torch.zeros(1, requires_grad=True)
    return w, b
```

#### 4. 建立回归模型

```python
def linear_model(x_train, w, b):
    """
    线性回归模型，返回模型拟合输出y，这里用到了广播机制
    :param x_train: 特征矩阵(m,n)
    :param w: 参数矩阵(n, 1)
    :param b: 截距项
    :return: 拟合值y_hat(m, 1)
    """
    return torch.matmul(x_train, w) + b
```

#### 5. 计算损失函数

　　注意这里并没有直接将MSE加总，因此损失并不是一个标量。在后面进行梯度计算的时候，需要调用`sum()`得到标量，并进行反向传播来计算MSE对每个参数的梯度。

```python
def loss_function(y_hat, y):
    """
    根据拟合值和真实值，计算损失
    :param y_hat: 拟合值
    :param y: 真实值
    :return: 均方损失
    """
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2
```

#### 6. 梯度计算

　　之前我们初始化w和b时，设置了`required_grad=True`，使得当前的参数可以被保存梯度，当使用backward()的时候，参数的梯度会被保存在grad中。因此可以通过`param.grad`获得参数的梯度。在这里需要注意一下，这里我们使用了`with torch.no_grad()`，这里会调用一个上下文管理器，这部分代码并不会记录在参数的梯度中，因为我们参数的梯度是与MSE相关的，**不能被其他处理过程影响**。在for循环中，更新了梯度，最后`param.grad.zero_()`保证了参数的梯度不会保存（在默认情况下，PyTorch会累积梯度，我们需要清除之前的值）。

```python
ef sgd(params, lr, batch_size):
    """
    小批量随机梯度下降
    :param params: 参数列表，包括w,b
    :param lr: 学习率
    :param batch_size: 批量大小
    :return: None
    """
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()
```

#### 7. 线性回归训练

　　在每个_迭代周期_（epoch）中，我们使用`data_iter`函数遍历整个数据集， 并将训练数据集中所有样本都使用一次（假设样本数能够被批量大小整除）

```python
def linear_train(x_train, y_train, batch_size=10, lr=0.01, epochs=10000):
    """
    在10000次迭代过程中，不断更新w和b
    :param x_train: 训练数据集特征矩阵
    :param y_train: 训练数据集标签矩阵
    :param leanring_rate: 学习率
    :param epochs: 也就是训练次数
    :return: 记录每200次训练的损失losses，最终训练得到的参数params字典以及最后的梯度字典
    """
    losses = []
    w, b = initialize_parameters(x_train.shape[1])
    for epoch in range(epochs):
        for x_batch, y_batch in data_iter(batch_size, x_train, y_train):
            y_hat = linear_model(x_batch, w, b)
            loss = loss_function(y_hat, y_batch)
            # 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，
            # 并以此计算关于[w,b]的梯度
            loss.sum().backward()
            sgd([w, b], lr, batch_size)  # 使用参数的梯度更新参数
        if epoch % 200 == 0:
            with torch.no_grad():
                # 记录每个batch的loss
                train_l = loss_function(linear_model(x_batch, w, b), y_batch)
                losses.append(train_l)
                print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
    return losses, w, b
```

![image-20220518152743721](https://d/NoteBook\_md/images/image-20220518152743721.png)

## 3. 关于线性回归模型的优化

　　在机器学习模型中，可能会出现的两类问题：欠拟合和过拟合。针对欠拟合问题，我们可以考虑加强数据特征维度，使用更加复杂的算法。而对于过拟合问题，可以参考我之前的文章：[细说过拟合](https://mp.weixin.qq.com/s?\_\_biz=MzIxMjE3Nzg5Nw==\&mid=2247483882\&idx=1\&sn=1f2ceafa226bf854840a86383bb81438\&chksm=974b476ca03cce7a9fae2e8a421c9d6983887301a7701637447cec7fa931c432aa5ef706af84\&token=1610238453\&lang=zh\_CN#rd)。最后再总结一下线性回归模型的优缺点。

### （一）优点

1. 建模速度快，不需要很复杂的计算，在数据量大的情况下依然运行速度很快
2. 模型可解释性强。相对于很多机器学习的**黑盒模型**，线性回归可以根据系数给出每个变量的理解和解释

### （二）缺点

不能很好地拟合非线性数据。所以需要先判断变量之间是否是线性关系。

## 4. 小结

　　以上就是本文的全部内容。

参考资料：

[1.常用损失函数——机器之心](https://www.jiqizhixin.com/articles/2018-06-21-3)

[2.动手学深度学习——线性回归的从零开始实现](https://zh.d2l.ai/chapter\_linear-networks/linear-regression-scratch.html#id7)
